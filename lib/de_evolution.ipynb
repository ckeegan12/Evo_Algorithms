{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-17T23:25:38.198005Z",
          "iopub.status.busy": "2026-02-17T23:25:38.197716Z",
          "iopub.status.idle": "2026-02-17T23:25:38.208907Z",
          "shell.execute_reply": "2026-02-17T23:25:38.208259Z",
          "shell.execute_reply.started": "2026-02-17T23:25:38.197981Z"
        },
        "id": "viKo_ELK3wHK",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.autograd import Function\n",
        "import math\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import os\n",
        "import sys\n",
        "\n",
        "def adder2d_function(X, W, stride=1, padding=0):\n",
        "    n_filters, d_filter, h_filter, w_filter = W.size()\n",
        "    n_x, d_x, h_x, w_x = X.size()\n",
        "\n",
        "    h_out = (h_x - h_filter + 2 * padding) / stride + 1\n",
        "    w_out = (w_x - w_filter + 2 * padding) / stride + 1\n",
        "\n",
        "    h_out, w_out = int(h_out), int(w_out)\n",
        "    X_col = torch.nn.functional.unfold(X.view(1, -1, h_x, w_x), h_filter, dilation=1, padding=padding, stride=stride).view(n_x, -1, h_out*w_out)\n",
        "    X_col = X_col.permute(1,2,0).contiguous().view(X_col.size(1),-1)\n",
        "    W_col = W.view(n_filters, -1)\n",
        "    \n",
        "    out = adder.apply(W_col,X_col)\n",
        "    \n",
        "    out = out.view(n_filters, h_out, w_out, n_x)\n",
        "    out = out.permute(3, 0, 1, 2).contiguous()\n",
        "    \n",
        "    return out\n",
        "\n",
        "class adder(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, W_col, X_col):\n",
        "        ctx.save_for_backward(W_col,X_col)\n",
        "        output = -(W_col.unsqueeze(2)-X_col.unsqueeze(0)).abs().sum(1)\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx,grad_output):\n",
        "        W_col,X_col = ctx.saved_tensors\n",
        "        grad_W_col = ((X_col.unsqueeze(0)-W_col.unsqueeze(2))*grad_output.unsqueeze(1)).sum(2)\n",
        "        grad_W_col = grad_W_col/grad_W_col.norm(p=2).clamp(min=1e-12)*math.sqrt(W_col.size(1)*W_col.size(0))/5\n",
        "        grad_X_col = (-(X_col.unsqueeze(0)-W_col.unsqueeze(2)).clamp(-1,1)*grad_output.unsqueeze(1)).sum(0)\n",
        "        \n",
        "        return grad_W_col, grad_X_col\n",
        "    \n",
        "class adder2d(nn.Module):\n",
        "\n",
        "    def __init__(self,input_channel,output_channel,kernel_size, stride=1, padding=0, bias = False):\n",
        "        super(adder2d, self).__init__()\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.input_channel = input_channel\n",
        "        self.output_channel = output_channel\n",
        "        self.kernel_size = kernel_size\n",
        "        self.adder = torch.nn.Parameter(nn.init.normal_(torch.randn(output_channel,input_channel,kernel_size,kernel_size)))\n",
        "        self.bias = bias\n",
        "        if bias:\n",
        "            self.b = torch.nn.Parameter(nn.init.uniform_(torch.zeros(output_channel)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = adder2d_function(x,self.adder, self.stride, self.padding)\n",
        "        if self.bias:\n",
        "            output += self.b.unsqueeze(0).unsqueeze(2).unsqueeze(3)\n",
        "        \n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-17T23:25:38.210478Z",
          "iopub.status.busy": "2026-02-17T23:25:38.210271Z",
          "iopub.status.idle": "2026-02-17T23:25:38.227766Z",
          "shell.execute_reply": "2026-02-17T23:25:38.226968Z",
          "shell.execute_reply.started": "2026-02-17T23:25:38.210460Z"
        },
        "id": "hcYwfOZ93wHL",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "def conv3x3(in_planes, out_planes, stride=1):\n",
        "    \" 3x3 convolution with padding \"\n",
        "    return adder2d(in_planes, out_planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion=1\n",
        "\n",
        "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride = stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(planes, planes)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.downsample = downsample\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            residual = self.downsample(x)\n",
        "\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-17T23:25:38.228837Z",
          "iopub.status.busy": "2026-02-17T23:25:38.228575Z",
          "iopub.status.idle": "2026-02-17T23:25:38.241206Z",
          "shell.execute_reply": "2026-02-17T23:25:38.240531Z",
          "shell.execute_reply.started": "2026-02-17T23:25:38.228817Z"
        },
        "id": "nrDxsuM23wHM",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "\n",
        "    def __init__(self, block, layers, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.inplanes = 16\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.layer1 = self._make_layer(block, 16, layers[0])\n",
        "        self.layer2 = self._make_layer(block, 32, layers[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 64, layers[2], stride=2)\n",
        "        self.avgpool = nn.AvgPool2d(8, stride=1)\n",
        "        self.fc = nn.Conv2d(64 * block.expansion, num_classes, 1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(num_classes)\n",
        "        \n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.BatchNorm2d):\n",
        "                m.weight.data.fill_(1)\n",
        "                m.bias.data.zero_()\n",
        "         \n",
        "    def _make_layer(self, block, planes, blocks, stride=1):\n",
        "        downsample = None\n",
        "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
        "            downsample = nn.Sequential(\n",
        "                adder2d(self.inplanes, planes * block.expansion, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(planes * block.expansion)\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        layers.append(block(inplanes = self.inplanes, planes = planes, stride = stride, downsample = downsample))\n",
        "        self.inplanes = planes * block.expansion\n",
        "        for _ in range(1, blocks):\n",
        "            layers.append(block(inplanes = self.inplanes, planes = planes))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "\n",
        "        x = self.avgpool(x)\n",
        "        x = self.fc(x)\n",
        "        x = self.bn2(x)\n",
        "\n",
        "        return x.view(x.size(0), -1)\n",
        "\n",
        "\n",
        "def resnet20(**kwargs):\n",
        "    return ResNet(BasicBlock, [3, 3, 3], **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-17T23:25:38.354553Z",
          "iopub.status.busy": "2026-02-17T23:25:38.354138Z",
          "iopub.status.idle": "2026-02-17T23:25:38.364188Z",
          "shell.execute_reply": "2026-02-17T23:25:38.363646Z",
          "shell.execute_reply.started": "2026-02-17T23:25:38.354532Z"
        },
        "id": "yoUkVj1I3wHM",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "\n",
        "class AdderNet(nn.Module):\n",
        "    \"\"\"\n",
        "    AdderNet with Quantization Support\n",
        "\n",
        "    This is a wrapper around ResNet20 that adds:\n",
        "    - Manual weight loading for quantization\n",
        "    - Activation saving for debugging\n",
        "    - Classification method with softmax\n",
        "\n",
        "    Quantization Preprocessing:\n",
        "    - Adder weights → quantized integers: W_clip ∈ [-2^(q-1), 2^(q-1)-1]\n",
        "    - BatchNorm running_mean → adjusted: μ' = round(μ/δ) + Σ|W_q - W_clip|\n",
        "    - BatchNorm bias → quantized: β' = β/δ\n",
        "    - Final FC weights → scaled: W_fc' = W_fc * δ\n",
        "\n",
        "    During inference:\n",
        "    - Adder: outputs -Σ|X - W_clip| (integer operations)\n",
        "    - BatchNorm: Y = γ * (X - μ') / √(σ² + ε) + β'\n",
        "    - The weight bias is implicitly handled via the adjusted running_mean\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=10, load_weights=None):\n",
        "        super(AdderNet, self).__init__()\n",
        "        \n",
        "        # Use ResNet20 as the base model\n",
        "        self.model = resnet20(num_classes=num_classes)\n",
        "        \n",
        "        if load_weights is not None:\n",
        "            self.load_manual_weights(load_weights)\n",
        "\n",
        "        self.activations = {}\n",
        "\n",
        "    def load_manual_weights(self, weights_dict):\n",
        "        \"\"\"\n",
        "        Load quantized weights into the model.\n",
        "        Expected preprocessing:\n",
        "        1. Adder weights: quantized integers W_clip\n",
        "        2. BN running_mean: adjusted with weight bias (μ' = round(μ/δ) + bias_sum)\n",
        "        3. BN bias: quantized by delta (β' = β/δ)\n",
        "        4. BN weight: quantized by delta for bn1\n",
        "        5. FC weights: scaled by delta (W_fc' = W_fc * δ)\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            for name, param in self.model.named_parameters():\n",
        "                if name in weights_dict:\n",
        "                    weight_value = weights_dict[name]\n",
        "                    if weight_value.shape == param.shape:\n",
        "                        param.copy_(weight_value.to(param.device))\n",
        "\n",
        "            for name, buffer in self.model.named_buffers():\n",
        "                if name in weights_dict:\n",
        "                    buffer_value = weights_dict[name]\n",
        "                    if buffer_value.shape == buffer.shape:\n",
        "                        buffer.copy_(buffer_value.to(buffer.device))\n",
        "\n",
        "    def forward(self, x, save_activations=False):\n",
        "        if save_activations:\n",
        "            self.activations['input_activation'] = x.clone()\n",
        "\n",
        "        # Forward through the model\n",
        "        out = self.model(x)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def classification(self, x):\n",
        "        out = self.forward(x)\n",
        "        return F.softmax(out, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2026-02-17T23:25:38.365704Z",
          "iopub.status.busy": "2026-02-17T23:25:38.365450Z",
          "iopub.status.idle": "2026-02-17T23:25:38.382757Z",
          "shell.execute_reply": "2026-02-17T23:25:38.381975Z",
          "shell.execute_reply.started": "2026-02-17T23:25:38.365685Z"
        },
        "id": "h0UHbovD3wHN",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "class EvolutionAlgorithmBase:\n",
        "    \"\"\"\n",
        "    Base class for Evolution Algorithms.\n",
        "    \"\"\"\n",
        "    def __init__(self, func, n_dim, size_pop, max_iter, prob_mut):\n",
        "        self.func = func\n",
        "        self.n_dim = n_dim\n",
        "        self.size_pop = size_pop\n",
        "        self.max_iter = max_iter\n",
        "        self.prob_mut = prob_mut\n",
        "        \n",
        "        # History containers\n",
        "        self.generation_best_X = []\n",
        "        self.generation_best_Y = []\n",
        "        self.all_history_Y = []\n",
        "        self.all_history_FitV = []\n",
        "\n",
        "    def run(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "class DE(EvolutionAlgorithmBase):\n",
        "    \"\"\"\n",
        "    Differential Evolution (DE) Algorithm.\n",
        "    \n",
        "    This class implements the Differential Evolution algorithm for activation cutoff optimization.\n",
        "    It uses a loop-based approach to ensure distinct candidate selection for mutation.\n",
        "    This implementation maximizes the accuracy.\n",
        "    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    func: callable\n",
        "        The objective function to minimize. \n",
        "    n_dim: int\n",
        "        The dimension of the search space (Layers*blocks*2 + 2) = 20.\n",
        "    F: float\n",
        "        The mutation factor (differential weight).\n",
        "    size_pop: int\n",
        "        The size of the population.\n",
        "    max_iter: int\n",
        "        The maximum number of iterations/generations.\n",
        "    lb: array\n",
        "        Lower bounds for the activation values.\n",
        "    ub: array\n",
        "        Upper bounds for the activation values.\n",
        "    prob_mut: float (optional, default 0.7)\n",
        "        The crossover rate (CR).\n",
        "    \"\"\"\n",
        "    def __init__(self, func, F, lb, ub,\n",
        "                 size_pop, n_dim, max_iter, prob_mut):\n",
        "        # Note: 'prob_mut' corresponds to 'cr' (crossover rate) in sample code\n",
        "        super().__init__(func, n_dim, size_pop, max_iter, prob_mut)\n",
        "\n",
        "        self.F = F\n",
        "        self.lb = np.array(lb) * np.ones(self.n_dim)\n",
        "        self.ub = np.array(ub) * np.ones(self.n_dim)\n",
        "        \n",
        "        # Initialize population\n",
        "        self.crtbp()\n",
        "        # Evaluate initial population\n",
        "        self.Y = np.array([self.func(x) for x in self.X])\n",
        "\n",
        "    def crtbp(self):\n",
        "        \"\"\"Create the initial population\"\"\"\n",
        "        self.X = np.random.uniform(self.lb, self.ub, (self.size_pop, self.n_dim))\n",
        "        self.X = np.round(self.X, 2)\n",
        "        \n",
        "        return self.X\n",
        "\n",
        "    def mutation_op(self, x, F):\n",
        "        \"\"\"\n",
        "        Mutation operation: x[0] + F * (x[1] - x[2])\n",
        "        x is a list/array of 3 vectors [a, b, c]\n",
        "        \"\"\"\n",
        "        return x[0] + F * (x[1] - x[2])\n",
        "\n",
        "    def check_bounds(self, mutated):\n",
        "        \"\"\"Boundary check operation using clip\"\"\"\n",
        "        return np.clip(mutated, self.lb, self.ub)\n",
        "\n",
        "    def crossover_op(self, mutated, target, cr):\n",
        "        \"\"\"\n",
        "        Crossover operation\n",
        "        \"\"\"\n",
        "        # generate a uniform random value for every dimension\n",
        "        p = np.random.rand(self.n_dim)\n",
        "\n",
        "        # ensure at least one parameter is from mutated vector\n",
        "        j_rand = np.random.randint(0, self.n_dim)\n",
        "        \n",
        "        # Apply the crossover logic:\n",
        "        # Use mutant value if rand <= CR OR if it's the forced index\n",
        "        trial_vector = np.where((p <= cr) | (np.arange(self.n_dim) == j_rand), \n",
        "                                mutated, \n",
        "                                target)\n",
        "        # Round to hundredths place (2 decimal places)\n",
        "        trial_vector = np.round(trial_vector, 2)\n",
        "        return trial_vector\n",
        "\n",
        "    def run(self, max_iter=None):\n",
        "        self.max_iter = max_iter or self.max_iter\n",
        "        \n",
        "        # Initial Best\n",
        "        best_idx = np.argmax(self.Y)\n",
        "        self.best_x = self.X[best_idx].copy()\n",
        "        self.best_y = self.Y[best_idx]\n",
        "\n",
        "        for i in range(self.max_iter):\n",
        "            # Iterate over all candidate solutions\n",
        "            for j in range(self.size_pop):\n",
        "                # Choose three candidates a, b, c that are not the current one\n",
        "                # to ensure distinct indices for mutation\n",
        "                candidates = [idx for idx in range(self.size_pop) if idx != j]\n",
        "                a_idx, b_idx, c_idx = np.random.choice(candidates, 3, replace=False)\n",
        "                \n",
        "                a = self.X[a_idx]\n",
        "                b = self.X[b_idx]\n",
        "                c = self.X[c_idx]\n",
        "                \n",
        "                # Perform mutation\n",
        "                mutated = self.mutation_op([a, b, c], self.F)\n",
        "                \n",
        "                # Check bounds\n",
        "                mutated = self.check_bounds(mutated)\n",
        "                \n",
        "                # Perform crossover\n",
        "                trial = self.crossover_op(mutated, self.X[j], self.prob_mut)\n",
        "                \n",
        "                # Compute objective function value for trial vector\n",
        "                # (Assuming func takes a single vector)\n",
        "                if hasattr(self.func, 'batch_mode') and self.func.batch_mode:\n",
        "                     # Handle batch if necessary, but sample assumes single\n",
        "                     obj_trial = self.func(trial.reshape(1, -1))[0]\n",
        "                else:\n",
        "                     obj_trial = self.func(trial)\n",
        "                \n",
        "                print(f\"Trial {j} accuracy: {obj_trial:.2f}%\")\n",
        "                \n",
        "                obj_target = self.Y[j]\n",
        "                \n",
        "                # Perform selection\n",
        "                if obj_trial > obj_target:\n",
        "                    # Replace the target vector with the trial vector\n",
        "                    self.X[j] = trial\n",
        "                    self.Y[j] = obj_trial\n",
        "            \n",
        "            # Record the best individual of this generation\n",
        "            generation_best_index = np.argmax(self.Y)\n",
        "            current_best_y = self.Y[generation_best_index]\n",
        "            \n",
        "            self.generation_best_X.append(self.X[generation_best_index, :].copy())\n",
        "            self.generation_best_Y.append(current_best_y)\n",
        "            self.all_history_Y.append(self.Y.copy())\n",
        "            \n",
        "            print(f\"Generation {i+1}: Best Accuracy = {current_best_y:.2f}%\")\n",
        "            \n",
        "            # Update global best\n",
        "            if current_best_y > self.best_y:\n",
        "                 self.best_y = current_best_y\n",
        "                 self.best_x = self.X[generation_best_index].copy()\n",
        "\n",
        "        return self.best_x, self.best_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "def load_data(dataset='cifar10', data_dir='./data', batch_size=100, workers=4):\n",
        "    \"\"\"\n",
        "    Load test data for CIFAR10 or ImageNet\n",
        "    \n",
        "    Args:\n",
        "        dataset: 'cifar10' or 'ImageNet'\n",
        "        data_dir: path to dataset directory\n",
        "        batch_size: batch size for data loader\n",
        "        workers: number of data loading workers\n",
        "    \n",
        "    Returns:\n",
        "        testloader: PyTorch DataLoader for test set\n",
        "    \"\"\"\n",
        "    print(f'Preparing {dataset} data from {data_dir}..')\n",
        "    \n",
        "    if dataset.lower() == 'cifar10':\n",
        "        val_loader = torch.utils.data.DataLoader(\n",
        "            datasets.CIFAR10(data_dir, train=False, download=True, transform=transforms.Compose([\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
        "            ])),\n",
        "            batch_size=batch_size, shuffle=False,\n",
        "            num_workers=workers, pin_memory=True)\n",
        "    elif dataset.lower() == 'imagenet':\n",
        "        val_loader = torch.utils.data.DataLoader(\n",
        "            datasets.ImageFolder(data_dir, transforms.Compose([\n",
        "                transforms.Resize(256),\n",
        "                transforms.CenterCrop(224),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225])\n",
        "            ])),\n",
        "            batch_size=batch_size, shuffle=False,\n",
        "            num_workers=workers, pin_memory=True)\n",
        "    else:\n",
        "        raise ValueError(f\"Dataset {dataset} not supported. Choose 'cifar10' or 'ImageNet'\")\n",
        "    \n",
        "    return val_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "execution": {
          "iopub.execute_input": "2026-02-17T23:25:38.384071Z",
          "iopub.status.busy": "2026-02-17T23:25:38.383681Z"
        },
        "id": "83S-0xE53wHO",
        "outputId": "1ae4c08e-9ea5-43a5-8b17-7e8905f683f0",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
            "Starting DE Optimization for AdderNet2.0 Quantization\n",
            "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
            "Using device: cpu\n",
            "Preparing cifar10 data from ./data..\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "5.2%\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 224\u001b[39m\n\u001b[32m    221\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-+\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m25\u001b[39m)\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m224\u001b[39m     \u001b[43mrun_optimization\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 143\u001b[39m, in \u001b[36mrun_optimization\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    140\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    142\u001b[39m \u001b[38;5;66;03m# Load Data\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m testloader = \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcifar10\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./data\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[38;5;66;03m# Load Pretrained Model Weights\u001b[39;00m\n\u001b[32m    146\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mLoading pretrained weights...\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mload_data\u001b[39m\u001b[34m(dataset, data_dir, batch_size, workers)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mPreparing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m data from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m..\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dataset.lower() == \u001b[33m'\u001b[39m\u001b[33mcifar10\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     27\u001b[39m     val_loader = torch.utils.data.DataLoader(\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m         \u001b[43mdatasets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCIFAR10\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransforms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mToTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mNormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.4914\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.4822\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.4465\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.2023\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.1994\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.2010\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m     32\u001b[39m         batch_size=batch_size, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     33\u001b[39m         num_workers=workers, pin_memory=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m dataset.lower() == \u001b[33m'\u001b[39m\u001b[33mimagenet\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     35\u001b[39m     val_loader = torch.utils.data.DataLoader(\n\u001b[32m     36\u001b[39m         datasets.ImageFolder(data_dir, transforms.Compose([\n\u001b[32m     37\u001b[39m             transforms.Resize(\u001b[32m256\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m     43\u001b[39m         batch_size=batch_size, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     44\u001b[39m         num_workers=workers, pin_memory=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sledv\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torchvision\\datasets\\cifar.py:66\u001b[39m, in \u001b[36mCIFAR10.__init__\u001b[39m\u001b[34m(self, root, train, transform, target_transform, download)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;28mself\u001b[39m.train = train  \u001b[38;5;66;03m# training set or test set\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._check_integrity():\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mDataset not found or corrupted. You can use download=True to download it\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sledv\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torchvision\\datasets\\cifar.py:139\u001b[39m, in \u001b[36mCIFAR10.download\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._check_integrity():\n\u001b[32m    138\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[43mdownload_and_extract_archive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtgz_md5\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sledv\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torchvision\\datasets\\utils.py:388\u001b[39m, in \u001b[36mdownload_and_extract_archive\u001b[39m\u001b[34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[39m\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filename:\n\u001b[32m    386\u001b[39m     filename = os.path.basename(url)\n\u001b[32m--> \u001b[39m\u001b[32m388\u001b[39m \u001b[43mdownload_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    390\u001b[39m archive = os.path.join(download_root, filename)\n\u001b[32m    391\u001b[39m extract_archive(archive, extract_root, remove_finished)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sledv\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torchvision\\datasets\\utils.py:127\u001b[39m, in \u001b[36mdownload_url\u001b[39m\u001b[34m(url, root, filename, md5, max_redirect_hops)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# download the file\u001b[39;00m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m     \u001b[43m_urlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (urllib.error.URLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    129\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m url[:\u001b[32m5\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mhttps\u001b[39m\u001b[33m\"\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sledv\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torchvision\\datasets\\utils.py:30\u001b[39m, in \u001b[36m_urlretrieve\u001b[39m\u001b[34m(url, filename, chunk_size)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m urllib.request.urlopen(urllib.request.Request(url, headers={\u001b[33m\"\u001b[39m\u001b[33mUser-Agent\u001b[39m\u001b[33m\"\u001b[39m: USER_AGENT})) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[33m\"\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fh, tqdm(total=response.length, unit=\u001b[33m\"\u001b[39m\u001b[33mB\u001b[39m\u001b[33m\"\u001b[39m, unit_scale=\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m         \u001b[38;5;28;01mwhile\u001b[39;00m chunk := \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     31\u001b[39m             fh.write(chunk)\n\u001b[32m     32\u001b[39m             pbar.update(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sledv\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\http\\client.py:479\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt > \u001b[38;5;28mself\u001b[39m.length:\n\u001b[32m    477\u001b[39m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[32m    478\u001b[39m     amt = \u001b[38;5;28mself\u001b[39m.length\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m s = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[32m    481\u001b[39m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[32m    482\u001b[39m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[32m    483\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_conn()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sledv\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\socket.py:719\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot read from timed out object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    721\u001b[39m     \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sledv\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\ssl.py:1304\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1300\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1301\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1302\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1303\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1304\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1305\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1306\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sledv\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\ssl.py:1138\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "def get_adder_layer_keys(state_dict):\n",
        "    keys = []\n",
        "    for key in state_dict.keys():\n",
        "        if key.startswith('layer') and key.endswith('adder'):\n",
        "            keys.append(key)\n",
        "    return keys\n",
        "\n",
        "def quantization_objective(x, fixed_state_dict, adder_keys, bits, device, testloader):\n",
        "    \"\"\"\n",
        "    Objective function for DE.\n",
        "    x: array of max_activation_val scalars, one for each adder layer.\n",
        "    \"\"\"\n",
        "    quantized_state_dict = {k: v.clone() for k, v in fixed_state_dict.items()}\n",
        "\n",
        "    Max_A = 2**(bits) - 1\n",
        "    Max_B = 0\n",
        "\n",
        "    # Helper to calculate delta\n",
        "    def get_delta(max_val):\n",
        "        return max_val / Max_A\n",
        "\n",
        "    delta_first = get_delta(x[0])\n",
        "    delta_last = get_delta(x[-1])\n",
        "\n",
        "    # Quantize bn1\n",
        "    quantized_state_dict['bn1.weight'] = quantized_state_dict['bn1.weight'] / delta_first\n",
        "    quantized_state_dict['bn1.bias'] = quantized_state_dict['bn1.bias'] / delta_first\n",
        "\n",
        "    # Pre-calculate deltas for all adder layers\n",
        "    layer_deltas = {key: get_delta(val) for key, val in zip(adder_keys, x)}\n",
        "\n",
        "    bias_sums = {}\n",
        "\n",
        "    # Process layers sequentially\n",
        "    current_delta = delta_first\n",
        "    current_bias_sum = 0\n",
        "\n",
        "    for name in quantized_state_dict.keys():\n",
        "        if name in adder_keys:\n",
        "            # apply AOQ to weights\n",
        "            w_tensor = quantized_state_dict[name]\n",
        "            current_delta = layer_deltas[name]\n",
        "            wq = torch.round(w_tensor / current_delta)\n",
        "            wq_clamp = torch.clamp(wq, max=Max_A, min=Max_B)\n",
        "            quantized_state_dict[name] = wq_clamp\n",
        "\n",
        "            # Calculate bias sum for FBR\n",
        "            bias_tensor = (wq - wq_clamp).abs()\n",
        "            current_bias_sum = torch.sum(bias_tensor, dim=(1,2,3))\n",
        "\n",
        "        elif name.startswith('layer'):\n",
        "            # Handle BN parameters for layers (layer1, layer2, etc.)\n",
        "            # Assumes these come AFTER their corresponding adder layer\n",
        "            if name.endswith('running_mean'):\n",
        "                m_tensor = quantized_state_dict[name]\n",
        "                mq = torch.round(m_tensor / current_delta)\n",
        "                quantized_state_dict[name] = mq + current_bias_sum\n",
        "\n",
        "            elif name.endswith('bias') and 'bn' in name:\n",
        "                x_tensor = quantized_state_dict[name]\n",
        "                xq_tensor = x_tensor / current_delta\n",
        "                quantized_state_dict[name] = xq_tensor\n",
        "\n",
        "    # Quantize FC\n",
        "    quantized_state_dict['fc.weight'] = quantized_state_dict['fc.weight'] * delta_last\n",
        "\n",
        "    # Evaluate\n",
        "    quant_model = AdderNet(num_classes=10).to(device)\n",
        "    quant_model.load_manual_weights(quantized_state_dict)\n",
        "    quant_model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in testloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = quant_model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    accuracy = 100. * correct / total\n",
        "    return accuracy\n",
        "\n",
        "def run_optimization():\n",
        "    print(\"-+\" * 25)\n",
        "    print(\"Starting DE Optimization for AdderNet2.0 Quantization\")\n",
        "    print(\"-+\" * 25)\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load Data\n",
        "    testloader = load_data(dataset='cifar10', data_dir='./data', batch_size=200, workers=4)\n",
        "\n",
        "    # Load Pretrained Model Weights\n",
        "    print('Loading pretrained weights...')\n",
        "    #model_dir = '/kaggle/input/state-dictionary'\n",
        "    # files = os.listdir(model_dir)\n",
        "    #model_path = os.path.join(model_dir, 'AdderNet_model.pth')\n",
        "    model_path = 'AdderNet_model.pth'\n",
        "\n",
        "    # Load state dictionary\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    if 'net' in checkpoint:\n",
        "        state_dict_raw = checkpoint['net']\n",
        "    else:\n",
        "        state_dict_raw = checkpoint\n",
        "\n",
        "    def remap_key(key):\n",
        "        \"\"\"Map original checkpoint keys to the correct ResNet20 naming.\"\"\"\n",
        "        new_key = key.replace('module.', '')\n",
        "\n",
        "        # conv1, bn1, fc, bn2 are at the root level - keep them as is\n",
        "        if new_key.startswith('conv1.') or new_key.startswith('bn1.') or new_key.startswith('fc.') or new_key.startswith('bn2.'):\n",
        "            return new_key\n",
        "\n",
        "        # Process residual layers\n",
        "        # Original structure: layer1.0.conv1.adder -> New structure: layer1.0.conv1.adder\n",
        "        # The ResNet20 uses Sequential for layers, so keys are already in correct format\n",
        "        # We just need to remove 'module.' prefix\n",
        "        \n",
        "        return new_key\n",
        "\n",
        "    # Apply remapping\n",
        "    fixed_state_dict = {}\n",
        "    for k, v in state_dict_raw.items():\n",
        "        fixed_key = remap_key(k)\n",
        "        fixed_state_dict[fixed_key] = v\n",
        "\n",
        "    # Prepare keys\n",
        "    print(\"Debug: First 10 keys in fixed_state_dict:\")\n",
        "    for key in list(fixed_state_dict.keys())[:10]:\n",
        "        print(key)\n",
        "\n",
        "    adder_keys = get_adder_layer_keys(fixed_state_dict)\n",
        "    print(f\"Found {len(adder_keys)} adder layers to optimize.\")\n",
        "    if len(adder_keys) == 0:\n",
        "        print(\"Error: No adder layers found. Check model keys or filtering logic.\")\n",
        "        return\n",
        "\n",
        "    # DE Parameters\n",
        "    n_dim = 20\n",
        "    size_pop = 50\n",
        "    max_iter = 50\n",
        "    prob_mut = 0.85 # Also called CR\n",
        "    F = 0.4\n",
        "\n",
        "    lb = [2.0] * 20\n",
        "    ub = [4.0] * 20\n",
        "\n",
        "    # Activation value range: using best results from previous generations\n",
        "    # lb = [2.4, 2.73, 2.63, 2.4, 2.61, 2.45, 2.53, 2.48, 3.09, 2.5, 2.13, 2.36, 2.0, 2.52, 2.18, 2.0, 2.29, 2.21, 2.0, 2.38]\n",
        "    # ub = [2.4, 2.74, 2.67, 2.4, 2.61, 2.45, 2.68, 2.48, 3.10, 2.5, 2.13, 2.36, 2.0, 2.52, 2.18, 2.0, 2.29, 2.21, 2.0, 2.38]\n",
        "\n",
        "    bit_array = [4] # Bits to be tested (unsigned integer 4)\n",
        "\n",
        "    for bits in bit_array:\n",
        "        print(f\"\\nOptimizing for {bits}-bit quantization...\")\n",
        "\n",
        "        # Define objective wrapper\n",
        "        def objective(x):\n",
        "            acc = quantization_objective(x, fixed_state_dict, adder_keys, bits, device, testloader)\n",
        "            return acc # maximizing accuracy\n",
        "\n",
        "        de = DE(objective, F, lb, ub, size_pop, n_dim, max_iter, prob_mut)\n",
        "\n",
        "        best_x, best_acc = de.run()\n",
        "\n",
        "        print(f\"Best Max Vals for {bits}-bit: {best_x}\")\n",
        "        print(f\"Best Accuracy: {best_acc:.2f}%\")\n",
        "        print(\"-+\" * 25)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_optimization()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
            "Testing Specific Activation Clipping Values\n",
            "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
            "Using device: cpu\n",
            "Preparing cifar10 data from ./data..\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2.4%\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 58\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# Run the test\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[34m__name__\u001b[39m == \u001b[33m\"\u001b[39m\u001b[33m__main__\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     \u001b[43mtest_specific_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 11\u001b[39m, in \u001b[36mtest_specific_values\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUsing device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Load Data\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m testloader = \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcifar10\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m./data\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m200\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Load Pretrained Model Weights\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mLoading pretrained weights...\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 28\u001b[39m, in \u001b[36mload_data\u001b[39m\u001b[34m(dataset, data_dir, batch_size, workers)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mPreparing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m data from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdata_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m..\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     26\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dataset.lower() == \u001b[33m'\u001b[39m\u001b[33mcifar10\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     27\u001b[39m     val_loader = torch.utils.data.DataLoader(\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m         \u001b[43mdatasets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCIFAR10\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransforms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mToTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mNormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.4914\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.4822\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.4465\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.2023\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.1994\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.2010\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m     32\u001b[39m         batch_size=batch_size, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     33\u001b[39m         num_workers=workers, pin_memory=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m dataset.lower() == \u001b[33m'\u001b[39m\u001b[33mimagenet\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m     35\u001b[39m     val_loader = torch.utils.data.DataLoader(\n\u001b[32m     36\u001b[39m         datasets.ImageFolder(data_dir, transforms.Compose([\n\u001b[32m     37\u001b[39m             transforms.Resize(\u001b[32m256\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m     43\u001b[39m         batch_size=batch_size, shuffle=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m     44\u001b[39m         num_workers=workers, pin_memory=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sledv\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torchvision\\datasets\\cifar.py:66\u001b[39m, in \u001b[36mCIFAR10.__init__\u001b[39m\u001b[34m(self, root, train, transform, target_transform, download)\u001b[39m\n\u001b[32m     63\u001b[39m \u001b[38;5;28mself\u001b[39m.train = train  \u001b[38;5;66;03m# training set or test set\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m download:\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdownload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._check_integrity():\n\u001b[32m     69\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mDataset not found or corrupted. You can use download=True to download it\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sledv\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torchvision\\datasets\\cifar.py:139\u001b[39m, in \u001b[36mCIFAR10.download\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._check_integrity():\n\u001b[32m    138\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[43mdownload_and_extract_archive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mroot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtgz_md5\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sledv\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torchvision\\datasets\\utils.py:388\u001b[39m, in \u001b[36mdownload_and_extract_archive\u001b[39m\u001b[34m(url, download_root, extract_root, filename, md5, remove_finished)\u001b[39m\n\u001b[32m    385\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m filename:\n\u001b[32m    386\u001b[39m     filename = os.path.basename(url)\n\u001b[32m--> \u001b[39m\u001b[32m388\u001b[39m \u001b[43mdownload_url\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdownload_root\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmd5\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    390\u001b[39m archive = os.path.join(download_root, filename)\n\u001b[32m    391\u001b[39m extract_archive(archive, extract_root, remove_finished)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sledv\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torchvision\\datasets\\utils.py:127\u001b[39m, in \u001b[36mdownload_url\u001b[39m\u001b[34m(url, root, filename, md5, max_redirect_hops)\u001b[39m\n\u001b[32m    125\u001b[39m \u001b[38;5;66;03m# download the file\u001b[39;00m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m     \u001b[43m_urlretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (urllib.error.URLError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    129\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m url[:\u001b[32m5\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mhttps\u001b[39m\u001b[33m\"\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sledv\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torchvision\\datasets\\utils.py:30\u001b[39m, in \u001b[36m_urlretrieve\u001b[39m\u001b[34m(url, filename, chunk_size)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m urllib.request.urlopen(urllib.request.Request(url, headers={\u001b[33m\"\u001b[39m\u001b[33mUser-Agent\u001b[39m\u001b[33m\"\u001b[39m: USER_AGENT})) \u001b[38;5;28;01mas\u001b[39;00m response:\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(filename, \u001b[33m\"\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fh, tqdm(total=response.length, unit=\u001b[33m\"\u001b[39m\u001b[33mB\u001b[39m\u001b[33m\"\u001b[39m, unit_scale=\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m         \u001b[38;5;28;01mwhile\u001b[39;00m chunk := \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     31\u001b[39m             fh.write(chunk)\n\u001b[32m     32\u001b[39m             pbar.update(\u001b[38;5;28mlen\u001b[39m(chunk))\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sledv\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\http\\client.py:479\u001b[39m, in \u001b[36mHTTPResponse.read\u001b[39m\u001b[34m(self, amt)\u001b[39m\n\u001b[32m    476\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m amt > \u001b[38;5;28mself\u001b[39m.length:\n\u001b[32m    477\u001b[39m     \u001b[38;5;66;03m# clip the read to the \"end of response\"\u001b[39;00m\n\u001b[32m    478\u001b[39m     amt = \u001b[38;5;28mself\u001b[39m.length\n\u001b[32m--> \u001b[39m\u001b[32m479\u001b[39m s = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mamt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m s \u001b[38;5;129;01mand\u001b[39;00m amt:\n\u001b[32m    481\u001b[39m     \u001b[38;5;66;03m# Ideally, we would raise IncompleteRead if the content-length\u001b[39;00m\n\u001b[32m    482\u001b[39m     \u001b[38;5;66;03m# wasn't satisfied, but it might break compatibility.\u001b[39;00m\n\u001b[32m    483\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_conn()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sledv\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\socket.py:719\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot read from timed out object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    721\u001b[39m     \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sledv\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\ssl.py:1304\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1300\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1301\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1302\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1303\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1304\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1305\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1306\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\sledv\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\ssl.py:1138\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# Test specific activation clipping values\n",
        "def test_specific_values():\n",
        "    print(\"-+\" * 25)\n",
        "    print(\"Testing Specific Activation Clipping Values\")\n",
        "    print(\"-+\" * 25)\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load Data\n",
        "    testloader = load_data(dataset='cifar10', data_dir='./data', batch_size=200, workers=4)\n",
        "\n",
        "    # Load Pretrained Model Weights\n",
        "    print('Loading pretrained weights...')\n",
        "    model_path = 'AdderNet_model.pth'\n",
        "\n",
        "    # Load state dictionary\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    if 'net' in checkpoint:\n",
        "        state_dict_raw = checkpoint['net']\n",
        "    else:\n",
        "        state_dict_raw = checkpoint\n",
        "\n",
        "    def remap_key(key):\n",
        "        \"\"\"Map original checkpoint keys to the correct ResNet20 naming.\"\"\"\n",
        "        new_key = key.replace('module.', '')\n",
        "        if new_key.startswith('conv1.') or new_key.startswith('bn1.') or new_key.startswith('fc.') or new_key.startswith('bn2.'):\n",
        "            return new_key\n",
        "        return new_key\n",
        "\n",
        "    # Apply remapping\n",
        "    fixed_state_dict = {}\n",
        "    for k, v in state_dict_raw.items():\n",
        "        fixed_key = remap_key(k)\n",
        "        fixed_state_dict[fixed_key] = v\n",
        "\n",
        "    adder_keys = get_adder_layer_keys(fixed_state_dict)\n",
        "    print(f\"Found {len(adder_keys)} adder layers\")\n",
        "\n",
        "    # Specific activation clipping values to test (provided 18, adding 2 more based on pattern)\n",
        "    test_values = [2.50, 1.97, 1.75, 0.75, 2.471, 1.821, 3.389, 1.586, 2.801, 1.26, \n",
        "                   3.49, 2.161, 2.009, 2.826, 2.757, 1.305, 2.255, 1.021, 2.0, 2.50]\n",
        "    \n",
        "    print(f\"\\nTesting activation values: {test_values}\")\n",
        "    print(f\"Number of values: {len(test_values)}\")\n",
        "\n",
        "    bits = 4\n",
        "    accuracy = quantization_objective(test_values, fixed_state_dict, adder_keys, bits, device, testloader)\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Results for 4-bit quantization with specified activation values:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}%\")\n",
        "    print(f\"{'='*60}\\n\")\n",
        "\n",
        "\n",
        "# Run the test\n",
        "if __name__ == \"__main__\":\n",
        "    test_specific_values()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "H100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 9477871,
          "sourceId": 14820373,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31260,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
