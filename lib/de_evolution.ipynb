{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 14820373,
          "sourceType": "datasetVersion",
          "datasetId": 9477871
        }
      ],
      "dockerImageVersionId": 31260,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "H100"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.autograd import Function\n",
        "import math\n",
        "\n",
        "class adder2_0(Function):\n",
        "    \"\"\"\n",
        "    AdderNet 2.0 forward operation with FBR (Fusion Bias Removal)\n",
        "    \"\"\"\n",
        "    @staticmethod\n",
        "    def forward(ctx, W_col, X_col):\n",
        "        # W_col: (out_channels, in_channels*k*k) - already quantized integers\n",
        "        # X_col: (in_channels*k*k, locations*batch) - activations\n",
        "        ctx.save_for_backward(W_col, X_col)\n",
        "\n",
        "        # Core AdderNet operation: -Σ|W - X|\n",
        "        output = -(W_col.unsqueeze(2) - X_col.unsqueeze(0)).abs().sum(1)\n",
        "\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        W_col, X_col = ctx.saved_tensors\n",
        "\n",
        "        grad_W_col = ((X_col.unsqueeze(0) - W_col.unsqueeze(2)) * grad_output.unsqueeze(1)).sum(2)\n",
        "        grad_W_col = grad_W_col / grad_W_col.norm(p=2).clamp(min=1e-12) * math.sqrt(W_col.size(1) * W_col.size(0)) / 5\n",
        "        grad_X_col = (-(X_col.unsqueeze(0) - W_col.unsqueeze(2)).clamp(-1, 1) * grad_output.unsqueeze(1)).sum(0)\n",
        "\n",
        "        return grad_W_col, grad_X_col\n",
        "\n",
        "class adder2d2_0(nn.Module):\n",
        "    def __init__(self, input_channel, output_channel, kernel_size,\n",
        "                 stride=1, padding=0, bias=False):\n",
        "        super(adder2d2_0, self).__init__()\n",
        "        self.stride = stride\n",
        "        self.padding = padding\n",
        "        self.input_channel = input_channel\n",
        "        self.output_channel = output_channel\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "        # Weight parameter: (out_channels, in_channels, k, k)\n",
        "        # During training: contains float weights\n",
        "        # After FBR preprocessing: contains quantized integer weights\n",
        "        self.adder = torch.nn.Parameter(\n",
        "            nn.init.normal_(\n",
        "                torch.randn(output_channel, input_channel, kernel_size, kernel_size)\n",
        "            )\n",
        "        )\n",
        "\n",
        "        self.bias = bias\n",
        "        if bias:\n",
        "            self.b = torch.nn.Parameter(nn.init.uniform_(torch.zeros(output_channel)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: (batch, in_channels, H, W)\n",
        "        n_x, d_x, h_x, w_x = x.size()\n",
        "        n_filters = self.output_channel\n",
        "\n",
        "        # Calculate output dimensions\n",
        "        h_out = (h_x - self.kernel_size + 2 * self.padding) // self.stride + 1\n",
        "        w_out = (w_x - self.kernel_size + 2 * self.padding) // self.stride + 1\n",
        "\n",
        "        # Unfold input into columns\n",
        "        # X_col shape: (batch, in_channels*k*k, h_out*w_out)\n",
        "        X_col = torch.nn.functional.unfold(\n",
        "            x.view(1, -1, h_x, w_x),\n",
        "            self.kernel_size,\n",
        "            dilation=1,\n",
        "            padding=self.padding,\n",
        "            stride=self.stride\n",
        "        ).view(n_x, -1, h_out * w_out)\n",
        "\n",
        "        # Reshape: (in_channels*k*k, h_out*w_out*batch)\n",
        "        X_col = X_col.permute(1, 2, 0).contiguous().view(X_col.size(1), -1)\n",
        "\n",
        "        # Reshape weights: (out_channels, in_channels*k*k)\n",
        "        W_col = self.adder.view(n_filters, -1)\n",
        "\n",
        "        # Apply adder operation (NO quantization logic here!)\n",
        "        out = adder2_0.apply(W_col, X_col)\n",
        "\n",
        "        # Reshape output: (out_channels, h_out, w_out, batch) -> (batch, out_channels, h_out, w_out)\n",
        "        out = out.view(n_filters, h_out, w_out, n_x)\n",
        "        out = out.permute(3, 0, 1, 2).contiguous()\n",
        "\n",
        "        # Add bias if needed\n",
        "        if self.bias:\n",
        "            out += self.b.unsqueeze(0).unsqueeze(2).unsqueeze(3)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-17T23:25:38.197716Z",
          "iopub.execute_input": "2026-02-17T23:25:38.198005Z",
          "iopub.status.idle": "2026-02-17T23:25:38.208907Z",
          "shell.execute_reply.started": "2026-02-17T23:25:38.197981Z",
          "shell.execute_reply": "2026-02-17T23:25:38.208259Z"
        },
        "id": "viKo_ELK3wHK"
      },
      "outputs": [],
      "execution_count": 19
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "#from Adder2_0 import adder2d2_0\n",
        "\n",
        "class ResidualBlock2_0(nn.Module):\n",
        "    \"\"\"\n",
        "    Residual Block for AdderNet 2.0 with Fusion Bias Removal (FBR)\n",
        "\n",
        "    Uses standard BatchNorm2d since the FBR preprocessing (post_proc_act_quant.py)\n",
        "    adjusts the BatchNorm parameters offline. No dynamic weight bias adjustment needed.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3,\n",
        "                 stride=1, padding=1, downsample=None):\n",
        "        super(ResidualBlock2_0, self).__init__()\n",
        "\n",
        "        self.adder1 = adder2d2_0(in_channels, out_channels, kernel_size,\n",
        "                                 stride=stride, padding=padding, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        self.adder2 = adder2d2_0(out_channels, out_channels, kernel_size,\n",
        "                                 stride=1, padding=padding, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.downsample = downsample\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "\n",
        "        # First adder + BN + ReLU\n",
        "        out = self.adder1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        # Second adder + BN\n",
        "        out = self.adder2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        # Downsample residual if needed\n",
        "        if self.downsample is not None:\n",
        "            downsample_adder, downsample_bn = self.downsample\n",
        "            residual = downsample_adder(x)\n",
        "            residual = downsample_bn(residual)\n",
        "\n",
        "        # Add residual and apply ReLU\n",
        "        out += residual\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-17T23:25:38.210271Z",
          "iopub.execute_input": "2026-02-17T23:25:38.210478Z",
          "iopub.status.idle": "2026-02-17T23:25:38.227766Z",
          "shell.execute_reply.started": "2026-02-17T23:25:38.210460Z",
          "shell.execute_reply": "2026-02-17T23:25:38.226968Z"
        },
        "id": "hcYwfOZ93wHL"
      },
      "outputs": [],
      "execution_count": 20
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "#from block2_0 import ResidualBlock2_0\n",
        "#from Adder2_0 import adder2d2_0\n",
        "\n",
        "class Layer2_0(nn.Module):\n",
        "    \"\"\"\n",
        "    Layer composition for AdderNet 2.0 with Fusion Bias Removal (FBR)\n",
        "\n",
        "    Creates a sequence of residual blocks with adder operations.\n",
        "    Uses standard BatchNorm2d since FBR preprocessing handles bias fusion offline.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, num_blocks=3):\n",
        "        super(Layer2_0, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "\n",
        "        downsample = None\n",
        "        stride = 1\n",
        "\n",
        "        # Create downsample path if channel dimensions change\n",
        "        if in_channels != out_channels:\n",
        "            self.downsample_adder = adder2d2_0(in_channels, out_channels, kernel_size=1,\n",
        "                                                stride=2, padding=0, bias=False)\n",
        "            self.downsample_bn = nn.BatchNorm2d(out_channels)\n",
        "            downsample = (self.downsample_adder, self.downsample_bn)\n",
        "            stride = 2\n",
        "        else:\n",
        "            stride = 1\n",
        "\n",
        "        self.blocks = nn.ModuleList()\n",
        "\n",
        "        # First block (may have stride=2 for downsampling)\n",
        "        self.blocks.append(ResidualBlock2_0(in_channels=in_channels,\n",
        "                                            out_channels=out_channels,\n",
        "                                            stride=stride,\n",
        "                                            downsample=downsample,\n",
        "                                            ))\n",
        "\n",
        "        # Remaining blocks (stride=1)\n",
        "        for _ in range(num_blocks - 1):\n",
        "            self.blocks.append(ResidualBlock2_0(in_channels=out_channels,\n",
        "                                                out_channels=out_channels,\n",
        "                                                padding=1,\n",
        "                                                ))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = x\n",
        "        for block in self.blocks:\n",
        "            out = block(out)\n",
        "        return out"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-17T23:25:38.228575Z",
          "iopub.execute_input": "2026-02-17T23:25:38.228837Z",
          "iopub.status.idle": "2026-02-17T23:25:38.241206Z",
          "shell.execute_reply.started": "2026-02-17T23:25:38.228817Z",
          "shell.execute_reply": "2026-02-17T23:25:38.240531Z"
        },
        "id": "nrDxsuM23wHM"
      },
      "outputs": [],
      "execution_count": 21
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "#from layer2_0 import Layer2_0\n",
        "class AdderNet2_0(nn.Module):\n",
        "    \"\"\"\n",
        "    AdderNet 2.0 with Fusion Bias Removal (FBR)\n",
        "\n",
        "    FBR Preprocessing:\n",
        "    - Adder weights → quantized integers: W_clip ∈ [-2^(q-1), 2^(q-1)-1]\n",
        "    - BatchNorm running_mean → adjusted: μ' = round(μ/δ) + Σ|W_q - W_clip|\n",
        "    - BatchNorm bias → quantized: β' = β/δ\n",
        "    - Final FC weights → scaled: W_fc' = W_fc * δ\n",
        "\n",
        "    During inference:\n",
        "    - Adder: outputs -Σ|X - W_clip| (integer operations)\n",
        "    - BatchNorm: Y = γ * (X - μ') / √(σ² + ε) + β'\n",
        "    - The weight bias is implicitly handled via the adjusted running_mean\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes=10, load_weights=None):\n",
        "        super(AdderNet2_0, self).__init__()\n",
        "\n",
        "        # Initial convolution layer (standard conv, not quantized)\n",
        "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(16)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Quantized adder layers with FBR\n",
        "        self.layer1 = Layer2_0(16, 16, num_blocks=3)\n",
        "        self.layer2 = Layer2_0(16, 32, num_blocks=3)\n",
        "        self.layer3 = Layer2_0(32, 64, num_blocks=3)\n",
        "\n",
        "        # Fully connected layer\n",
        "        self.fc = nn.Conv2d(64, num_classes, 1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(num_classes)\n",
        "\n",
        "        if load_weights is not None:\n",
        "            self.load_manual_weights(load_weights)\n",
        "\n",
        "        self.activations = {}\n",
        "\n",
        "    def load_manual_weights(self, weights_dict):\n",
        "        \"\"\"\n",
        "        Expected preprocessing:\n",
        "        1. Adder weights: quantized integers W_clip\n",
        "        2. BN running_mean: adjusted with weight bias (μ' = round(μ/δ) + bias_sum)\n",
        "        3. BN bias: quantized by delta (β' = β/δ)\n",
        "        4. BN weight: quantized by delta (γ' = γ/δ) for bn1 only\n",
        "        5. FC weights: scaled by delta (W_fc' = W_fc * δ)\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            for name, param in self.named_parameters():\n",
        "                if name in weights_dict:\n",
        "                    weight_value = weights_dict[name]\n",
        "                    if weight_value.shape == param.shape:\n",
        "                        param.copy_(weight_value.to(param.device))\n",
        "\n",
        "            for name, buffer in self.named_buffers():\n",
        "                if name in weights_dict:\n",
        "                    buffer_value = weights_dict[name]\n",
        "                    if buffer_value.shape == buffer.shape:\n",
        "                        buffer.copy_(buffer_value.to(buffer.device))\n",
        "\n",
        "    def forward(self, x, save_activations=False):\n",
        "        if save_activations:\n",
        "            self.activations['input_activation_2.0'] = x.clone()\n",
        "\n",
        "        # Initial conv + BN + ReLU\n",
        "        # Note: bn1.weight and bn1.bias are divided by delta\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        if save_activations:\n",
        "            self.activations['prelayer_activation_2.0'] = out.clone()\n",
        "\n",
        "        # Quantized adder layers with FBR\n",
        "        out = self.layer1(out)\n",
        "        if save_activations:\n",
        "            self.activations['layer1_activation_2.0'] = out.clone()\n",
        "\n",
        "        out = self.layer2(out)\n",
        "        if save_activations:\n",
        "            self.activations['layer2_activation_2.0'] = out.clone()\n",
        "\n",
        "        out = self.layer3(out)\n",
        "        if save_activations:\n",
        "            self.activations['layer3_activation_2.0'] = out.clone()\n",
        "\n",
        "        # Global average pooling\n",
        "        out = F.avg_pool2d(out, 8)\n",
        "\n",
        "        # Final FC layer (weights scaled by delta in preprocessing)\n",
        "        out = self.fc(out)\n",
        "        out = self.bn2(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def classification(self, x):\n",
        "        out = self.forward(x)\n",
        "        return F.softmax(out, dim=1)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-17T23:25:38.354138Z",
          "iopub.execute_input": "2026-02-17T23:25:38.354553Z",
          "iopub.status.idle": "2026-02-17T23:25:38.364188Z",
          "shell.execute_reply.started": "2026-02-17T23:25:38.354532Z",
          "shell.execute_reply": "2026-02-17T23:25:38.363646Z"
        },
        "id": "yoUkVj1I3wHM"
      },
      "outputs": [],
      "execution_count": 22
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "class EvolutionAlgorithmBase:\n",
        "    \"\"\"\n",
        "    Base class for Evolution Algorithms.\n",
        "    \"\"\"\n",
        "    def __init__(self, func, n_dim, size_pop, max_iter, prob_mut):\n",
        "        self.func = func\n",
        "        self.n_dim = n_dim\n",
        "        self.size_pop = size_pop\n",
        "        self.max_iter = max_iter\n",
        "        self.prob_mut = prob_mut\n",
        "\n",
        "        # History containers\n",
        "        self.generation_best_X = []\n",
        "        self.generation_best_Y = []\n",
        "        self.all_history_Y = []\n",
        "        self.all_history_FitV = []\n",
        "\n",
        "    def run(self):\n",
        "        pass\n",
        "\n",
        "\n",
        "class DE(EvolutionAlgorithmBase):\n",
        "    \"\"\"\n",
        "    Differential Evolution (DE) Algorithm.\n",
        "\n",
        "    This class implements the Differential Evolution algorithm for activation cutoff optimization.\n",
        "    It uses a loop-based approach to ensure distinct candidate selection for mutation.\n",
        "    This implementation maximizes the accuracy.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    func: callable\n",
        "        The objective function to minimize.\n",
        "    n_dim: int\n",
        "        The dimension of the search space (Layers*blocks*2 + 2) = 20.\n",
        "    F: float\n",
        "        The mutation factor (differential weight).\n",
        "    size_pop: int\n",
        "        The size of the population.\n",
        "    max_iter: int\n",
        "        The maximum number of iterations/generations.\n",
        "    lb: array\n",
        "        Lower bounds for the activation values.\n",
        "    ub: array\n",
        "        Upper bounds for the activation values.\n",
        "    prob_mut: float (optional, default 0.7)\n",
        "        The crossover rate (CR).\n",
        "    \"\"\"\n",
        "    def __init__(self, func, F, lb, ub,\n",
        "                 size_pop, n_dim, max_iter, prob_mut):\n",
        "        # Note: 'prob_mut' corresponds to 'cr' (crossover rate) in sample code\n",
        "        super().__init__(func, n_dim, size_pop, max_iter, prob_mut)\n",
        "\n",
        "        self.F = F\n",
        "        self.lb = np.array(lb) * np.ones(self.n_dim)\n",
        "        self.ub = np.array(ub) * np.ones(self.n_dim)\n",
        "\n",
        "        # Initialize population\n",
        "        self.crtbp()\n",
        "        # Evaluate initial population\n",
        "        self.Y = np.array([self.func(x) for x in self.X])\n",
        "\n",
        "    def crtbp(self):\n",
        "      \"\"\"Create the initial population\"\"\"\n",
        "      # de.X = np.random.uniform(self.lb, self.ub, (self.size_pop, self.n_dim))\n",
        "      # de.X = np.round(de.X, 2)\n",
        "\n",
        "      self.X = np.array([\n",
        "          [2.4, 2.74, 2.64, 2.4, 2.61, 2.45, 2.67, 2.48, 3.1, 2.5, 2.13, 2.36, 2., 2.52, 2.18, 2., 2.29, 2.21, 2., 2.38],\n",
        "          [2.4, 2.74, 2.64, 2.4, 2.61, 2.45, 2.59, 2.48, 3.1, 2.5, 2.13, 2.36, 2., 2.52, 2.18, 2., 2.29, 2.21, 2., 2.38],\n",
        "          [2.4, 2.74, 2.64, 2.4, 2.61, 2.45, 2.61, 2.48, 3.1, 2.5, 2.13, 2.36, 2., 2.52, 2.18, 2., 2.29, 2.21, 2., 2.38],\n",
        "          [2.4, 2.73, 2.64, 2.4, 2.61, 2.45, 2.62, 2.48, 3.1, 2.5, 2.13, 2.36, 2., 2.52, 2.18, 2., 2.29, 2.21, 2., 2.38],\n",
        "          [2.4, 2.73, 2.65, 2.4, 2.61, 2.45, 2.69, 2.48, 3.1, 2.5, 2.13, 2.36, 2., 2.52, 2.18, 2., 2.29, 2.21, 2., 2.38],\n",
        "          [2.4, 2.74, 2.63, 2.4, 2.61, 2.45, 2.59, 2.48, 3.1, 2.5, 2.13, 2.36, 2., 2.52, 2.18, 2., 2.29, 2.21, 2., 2.38],\n",
        "          [2.4, 2.73, 2.66, 2.4, 2.61, 2.45, 2.65, 2.48, 3.09, 2.5, 2.13, 2.36, 2., 2.52, 2.18, 2., 2.29, 2.21, 2., 2.38],\n",
        "          [2.4, 2.74, 2.64, 2.4, 2.61, 2.45, 2.67, 2.48, 3.1, 2.5, 2.13, 2.36, 2., 2.52, 2.18, 2., 2.29, 2.21, 2., 2.38],\n",
        "          [2.4, 2.74, 2.64, 2.4, 2.61, 2.45, 2.65, 2.48, 3.1, 2.5, 2.13, 2.36, 2., 2.52, 2.18, 2., 2.29, 2.21, 2., 2.38],\n",
        "          [2.4, 2.74, 2.64, 2.4, 2.61, 2.45, 2.6, 2.48, 3.1, 2.5, 2.13, 2.36, 2., 2.52, 2.18, 2., 2.29, 2.21, 2., 2.38],\n",
        "          [2.4, 2.73, 2.66, 2.4, 2.61, 2.45, 2.65, 2.48, 3.1, 2.5, 2.13, 2.36, 2., 2.52, 2.18, 2., 2.29, 2.21, 2., 2.38],\n",
        "          [2.4, 2.74, 2.7, 2.4, 2.61, 2.45, 2.53, 2.48, 3.1, 2.5, 2.13, 2.36, 2., 2.52, 2.18, 2., 2.29, 2.21, 2., 2.38],\n",
        "          [2.4, 2.74, 2.64, 2.4, 2.61, 2.45, 2.73, 2.48, 3.1, 2.5, 2.13, 2.36, 2., 2.52, 2.18, 2., 2.29, 2.21, 2., 2.38],\n",
        "          [2.4, 2.74, 2.66, 2.4, 2.61, 2.45, 2.69, 2.48, 3.1, 2.5, 2.13, 2.36, 2., 2.52, 2.18, 2., 2.29, 2.21, 2., 2.38],\n",
        "          [2.4, 2.73, 2.66, 2.4, 2.61, 2.45, 2.68, 2.48, 3.1, 2.5, 2.13, 2.36, 2., 2.52, 2.18, 2., 2.29, 2.21, 2., 2.38],\n",
        "          [2.4, 2.74, 2.63, 2.4, 2.61, 2.45, 2.64, 2.48, 3.1, 2.5, 2.13, 2.36, 2., 2.52, 2.18, 2., 2.29, 2.21, 2., 2.38],\n",
        "          [2.4, 2.74, 2.63, 2.4, 2.61, 2.45, 2.59, 2.48, 3.1, 2.5, 2.13, 2.36, 2., 2.52, 2.18, 2., 2.29, 2.21, 2., 2.38],\n",
        "          [2.4, 2.73, 2.63, 2.4, 2.61, 2.45, 2.65, 2.48, 3.1, 2.5, 2.13, 2.36, 2., 2.52, 2.18, 2., 2.29, 2.21, 2., 2.38],\n",
        "          [2.4, 2.73, 2.63, 2.4, 2.61, 2.45, 2.64, 2.48, 3.09, 2.5, 2.13, 2.36, 2., 2.52, 2.18, 2., 2.29, 2.21, 2., 2.38],\n",
        "          [2.4, 2.74, 2.66, 2.4, 2.61, 2.45, 2.53, 2.48, 3.1, 2.5, 2.13, 2.36, 2., 2.52, 2.18, 2., 2.29, 2.21, 2., 2.38],\n",
        "          [2.4, 2.74, 2.67, 2.4, 2.61, 2.45, 2.56, 2.48, 3.1, 2.5, 2.13, 2.36, 2., 2.52, 2.18, 2., 2.29, 2.21, 2., 2.38],\n",
        "          [2.4, 2.74, 2.64, 2.4, 2.61, 2.45, 2.62, 2.48, 3.09, 2.5, 2.13, 2.36, 2., 2.52, 2.18, 2., 2.29, 2.21, 2., 2.38],\n",
        "          [2.4, 2.74, 2.64, 2.4, 2.61, 2.45, 2.62, 2.48, 3.1, 2.5, 2.13, 2.36, 2., 2.52, 2.18, 2., 2.29, 2.21, 2., 2.38],\n",
        "          [2.4, 2.74, 2.64, 2.4, 2.61, 2.45, 2.62, 2.48, 3.1, 2.5, 2.13, 2.36, 2., 2.52, 2.18, 2., 2.29, 2.21, 2., 2.38],\n",
        "          [2.4, 2.74, 2.64, 2.4, 2.61, 2.45, 2.65, 2.48, 3.1, 2.5, 2.13, 2.36, 2., 2.52, 2.18, 2., 2.29, 2.21, 2., 2.38],\n",
        "          [2.4, 2.74, 2.66, 2.4, 2.61, 2.45, 2.59, 2.48, 3.1, 2.5, 2.13, 2.36, 2., 2.52, 2.18, 2., 2.29, 2.21, 2., 2.38],\n",
        "          [2.4, 2.74, 2.63, 2.4, 2.61, 2.45, 2.64, 2.48, 3.1, 2.5, 2.13, 2.36, 2., 2.52, 2.18, 2., 2.29, 2.21, 2., 2.38],\n",
        "          [2.4, 2.74, 2.66, 2.4, 2.61, 2.45, 2.62, 2.48, 3.1, 2.5, 2.13, 2.36, 2., 2.52, 2.18, 2., 2.29, 2.21, 2., 2.38],\n",
        "          [2.4, 2.74, 2.66, 2.4, 2.61, 2.45, 2.6, 2.48, 3.1, 2.5, 2.13, 2.36, 2., 2.52, 2.18, 2., 2.29, 2.21, 2., 2.38],\n",
        "          [2.4, 2.74, 2.64, 2.4, 2.61, 2.45, 2.6, 2.48, 3.09, 2.5, 2.13, 2.36, 2., 2.52, 2.18, 2., 2.29, 2.21, 2., 2.38],\n",
        "          [2.4, 2.74, 2.69, 2.4, 2.61, 2.45, 2.53, 2.48, 3.1, 2.5, 2.13, 2.36, 2., 2.52, 2.18, 2., 2.29, 2.21, 2., 2.38],\n",
        "          [2.4, 2.74, 2.64, 2.4, 2.61, 2.45, 2.6, 2.48, 3.1, 2.5, 2.13, 2.36, 2., 2.52, 2.18, 2., 2.29, 2.21, 2., 2.38],\n",
        "          [2.4, 2.74, 2.66, 2.4, 2.61, 2.45, 2.56, 2.48, 3.09, 2.5, 2.13, 2.36, 2., 2.52, 2.18, 2., 2.29, 2.21, 2., 2.38],\n",
        "          [2.4, 2.74, 2.67, 2.4, 2.61, 2.45, 2.67, 2.48, 3.1, 2.5, 2.13, 2.36, 2., 2.52, 2.18, 2., 2.29, 2.21, 2., 2.38],\n",
        "          [2.4, 2.73, 2.64, 2.4, 2.61, 2.45, 2.69, 2.48, 3.09, 2.5, 2.13, 2.36, 2., 2.52, 2.18, 2., 2.29, 2.21, 2., 2.38],\n",
        "          [2.4, 2.74, 2.66, 2.4, 2.61, 2.45, 2.67, 2.48, 3.09, 2.5, 2.13, 2.36, 2., 2.52, 2.18, 2., 2.29, 2.21, 2., 2.38],\n",
        "          [2.4, 2.74, 2.65, 2.4, 2.61, 2.45, 2.67, 2.48, 3.1, 2.5, 2.13, 2.36, 2., 2.52, 2.18, 2., 2.29, 2.21, 2., 2.38],\n",
        "          [2.4, 2.74, 2.67, 2.4, 2.61, 2.45, 2.53, 2.48, 3.09, 2.5, 2.13, 2.36, 2., 2.52, 2.18, 2., 2.29, 2.21, 2., 2.38],\n",
        "          [2.4, 2.74, 2.63, 2.4, 2.61, 2.45, 2.67, 2.48, 3.1, 2.5, 2.13, 2.36, 2., 2.52, 2.18, 2., 2.29, 2.21, 2., 2.38],\n",
        "          [2.4, 2.73, 2.66, 2.4, 2.61, 2.45, 2.67, 2.48, 3.09, 2.5, 2.13, 2.36, 2., 2.52, 2.18, 2., 2.29, 2.21, 2., 2.38],\n",
        "          [2.4, 2.73, 2.66, 2.4, 2.61, 2.45, 2.65, 2.48, 3.1, 2.5, 2.13, 2.36, 2., 2.52, 2.18, 2., 2.29, 2.21, 2., 2.38],\n",
        "          [2.4, 2.74, 2.66, 2.4, 2.61, 2.45, 2.65, 2.48, 3.09, 2.5, 2.13, 2.36, 2., 2.52, 2.18, 2., 2.29, 2.21, 2., 2.38],\n",
        "          [2.4, 2.74, 2.65, 2.4, 2.61, 2.45, 2.54, 2.48, 3.09, 2.5, 2.13, 2.36, 2., 2.52, 2.18, 2., 2.29, 2.21, 2., 2.38],\n",
        "          [2.4, 2.74, 2.64, 2.4, 2.61, 2.45, 2.69, 2.48, 3.1, 2.5, 2.13, 2.36, 2., 2.52, 2.18, 2., 2.29, 2.21, 2., 2.38],\n",
        "          [2.4, 2.73, 2.68, 2.4, 2.61, 2.45, 2.67, 2.48, 3.09, 2.5, 2.13, 2.36, 2., 2.52, 2.18, 2., 2.29, 2.21, 2., 2.38],\n",
        "          [2.4, 2.74, 2.65, 2.4, 2.61, 2.45, 2.69, 2.48, 3.1, 2.5, 2.13, 2.36, 2., 2.52, 2.18, 2., 2.29, 2.21, 2., 2.38],\n",
        "          [2.4, 2.74, 2.68, 2.4, 2.61, 2.45, 2.62, 2.48, 3.1, 2.5, 2.13, 2.36, 2., 2.52, 2.18, 2., 2.29, 2.21, 2., 2.38],\n",
        "          [2.4, 2.74, 2.66, 2.4, 2.61, 2.45, 2.58, 2.48, 3.1, 2.5, 2.13, 2.36, 2., 2.52, 2.18, 2., 2.29, 2.21, 2., 2.38],\n",
        "          [2.4, 2.74, 2.64, 2.4, 2.61, 2.45, 2.6, 2.48, 3.1, 2.5, 2.13, 2.36, 2., 2.52, 2.18, 2., 2.29, 2.21, 2., 2.38],\n",
        "          [2.4, 2.74, 2.64, 2.4, 2.61, 2.45, 2.61, 2.48, 3.1, 2.5, 2.13, 2.36, 2., 2.52, 2.18, 2., 2.29, 2.21, 2., 2.38],\n",
        "      ])\n",
        "      return self.X\n",
        "\n",
        "    def mutation_op(self, x, F):\n",
        "        \"\"\"\n",
        "        Mutation operation: x[0] + F * (x[1] - x[2])\n",
        "        x is a list/array of 3 vectors [a, b, c]\n",
        "        \"\"\"\n",
        "        return x[0] + F * (x[1] - x[2])\n",
        "\n",
        "    def check_bounds(self, mutated):\n",
        "        \"\"\"Boundary check operation using clip\"\"\"\n",
        "        return np.clip(mutated, self.lb, self.ub)\n",
        "\n",
        "    def crossover_op(self, mutated, target, cr):\n",
        "        \"\"\"\n",
        "        Crossover operation\n",
        "        \"\"\"\n",
        "        # generate a uniform random value for every dimension\n",
        "        p = np.random.rand(self.n_dim)\n",
        "\n",
        "        # ensure at least one parameter is from mutated vector\n",
        "        j_rand = np.random.randint(0, self.n_dim)\n",
        "\n",
        "        # Apply the crossover logic:\n",
        "        # Use mutant value if rand <= CR OR if it's the forced index\n",
        "        trial_vector = np.where((p <= cr) | (np.arange(self.n_dim) == j_rand),\n",
        "                                mutated,\n",
        "                                target)\n",
        "        # Round to hundredths place (2 decimal places)\n",
        "        trial_vector = np.round(trial_vector, 2)\n",
        "        return trial_vector\n",
        "\n",
        "    def run(self, max_iter=None):\n",
        "        self.max_iter = max_iter or self.max_iter\n",
        "        print(\"start run\")\n",
        "        # Initial Best\n",
        "        best_idx = np.argmax(self.Y)\n",
        "        self.best_x = self.X[best_idx].copy()\n",
        "        self.best_y = self.Y[best_idx]\n",
        "\n",
        "        for i in range(self.max_iter):\n",
        "            # Iterate over all candidate solutions\n",
        "            for j in range(self.size_pop):\n",
        "                # Choose three candidates a, b, c that are not the current one\n",
        "                # to ensure distinct indices for mutation\n",
        "                candidates = [idx for idx in range(self.size_pop) if idx != j]\n",
        "                a_idx, b_idx, c_idx = np.random.choice(candidates, 3, replace=False)\n",
        "\n",
        "                a = self.X[a_idx]\n",
        "                b = self.X[b_idx]\n",
        "                c = self.X[c_idx]\n",
        "                # Perform mutation\n",
        "                mutated = self.mutation_op([a, b, c], self.F)\n",
        "\n",
        "                # Check bounds\n",
        "                mutated = self.check_bounds(mutated)\n",
        "                # Perform crossover\n",
        "                trial = self.crossover_op(mutated, self.X[j], self.prob_mut)\n",
        "                print(f\"trial: {trial}\")\n",
        "\n",
        "                # Compute objective function value for trial vector\n",
        "                # (Assuming func takes a single vector)\n",
        "                if hasattr(self.func, 'batch_mode') and self.func.batch_mode:\n",
        "                     # Handle batch if necessary, but sample assumes single\n",
        "                     obj_trial = self.func(trial.reshape(1, -1))[0]\n",
        "                else:\n",
        "                     obj_trial = self.func(trial)\n",
        "\n",
        "                print(f\"Trial {j} accuracy: {obj_trial:.2f}%\")\n",
        "\n",
        "                obj_target = self.Y[j]\n",
        "\n",
        "                # Perform selection\n",
        "                if obj_trial > obj_target:\n",
        "                    # Replace the target vector with the trial vector\n",
        "                    self.X[j] = trial\n",
        "                    self.Y[j] = obj_trial\n",
        "\n",
        "            # Record the best individual of this generation\n",
        "            generation_best_index = np.argmax(self.Y)\n",
        "            current_best_y = self.Y[generation_best_index]\n",
        "\n",
        "            # Store history\n",
        "            self.generation_best_X.append(self.X[generation_best_index, :].copy())\n",
        "            self.generation_best_Y.append(current_best_y)\n",
        "            self.all_history_Y.append(self.Y.copy())\n",
        "\n",
        "            # Print progress\n",
        "            print(f\"Generation {i+1}: Best Accuracy = {current_best_y:.2f}%\")\n",
        "\n",
        "            # Update global best\n",
        "            if current_best_y > self.best_y:\n",
        "                 self.best_y = current_best_y\n",
        "                 self.best_x = self.X[generation_best_index].copy()\n",
        "                        # Elitism: Preserve top 3 solutions with accuracy > 10% for next generation\n",
        "            if i < self.max_iter - 1:  # Don't need to preserve on last iteration\n",
        "                # Find indices where accuracy > 10%\n",
        "                good_indices = np.where(self.Y > 10.0)[0]\n",
        "\n",
        "                if len(good_indices) > 0:\n",
        "                    # Sort by accuracy (descending) and take top 3\n",
        "                    sorted_good_indices = good_indices[np.argsort(self.Y[good_indices])[::-1]]\n",
        "                    elite_count = min(3, len(sorted_good_indices))\n",
        "                    elite_indices = sorted_good_indices[:elite_count]\n",
        "\n",
        "                    # Store elite solutions\n",
        "                    elite_X = self.X[elite_indices].copy()\n",
        "                    elite_Y = self.Y[elite_indices].copy()\n",
        "\n",
        "                    # After evolution completes for this generation, inject elites into population\n",
        "                    # Replace the worst solutions with the elite solutions\n",
        "                    worst_indices = np.argsort(self.Y)[:elite_count]\n",
        "                    self.X[worst_indices] = elite_X\n",
        "                    self.Y[worst_indices] = elite_Y\n",
        "\n",
        "        return self.best_x, self.best_y"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-17T23:25:38.365450Z",
          "iopub.execute_input": "2026-02-17T23:25:38.365704Z",
          "iopub.status.idle": "2026-02-17T23:25:38.382757Z",
          "shell.execute_reply.started": "2026-02-17T23:25:38.365685Z",
          "shell.execute_reply": "2026-02-17T23:25:38.381975Z"
        },
        "id": "h0UHbovD3wHN"
      },
      "outputs": [],
      "execution_count": 23
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "\n",
        "def load_data(batch_size=100):\n",
        "    print('Preparing data..')\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])\n",
        "\n",
        "    testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
        "    return testloader\n",
        "\n",
        "def get_adder_layer_keys(state_dict):\n",
        "    keys = []\n",
        "    for key in state_dict.keys():\n",
        "        if key.startswith('layer') and key.endswith('adder'):\n",
        "            keys.append(key)\n",
        "    return keys\n",
        "\n",
        "def quantization_objective(x, fixed_state_dict, adder_keys, bits, device, testloader):\n",
        "    \"\"\"\n",
        "    Objective function for DE.\n",
        "    x: array of max_activation_val scalars, one for each adder layer.\n",
        "    \"\"\"\n",
        "    quantized_state_dict = {k: v.clone() for k, v in fixed_state_dict.items()}\n",
        "\n",
        "    Max_A = 2**(bits) - 1\n",
        "    Max_B = 0\n",
        "\n",
        "    # Helper to calculate delta\n",
        "    def get_delta(max_val):\n",
        "        return max_val / Max_A\n",
        "\n",
        "    delta_first = get_delta(x[0])\n",
        "    delta_last = get_delta(x[-1])\n",
        "\n",
        "    # Quantize bn1\n",
        "    quantized_state_dict['bn1.weight'] = quantized_state_dict['bn1.weight'] / delta_first\n",
        "    quantized_state_dict['bn1.bias'] = quantized_state_dict['bn1.bias'] / delta_first\n",
        "\n",
        "    # Pre-calculate deltas for all adder layers\n",
        "    layer_deltas = {key: get_delta(val) for key, val in zip(adder_keys, x)}\n",
        "\n",
        "    bias_sums = {}\n",
        "\n",
        "    # Process layers sequentially\n",
        "    current_delta = delta_first\n",
        "    current_bias_sum = 0\n",
        "\n",
        "    for name in quantized_state_dict.keys():\n",
        "        if name in adder_keys:\n",
        "            # apply AOQ to weights\n",
        "            w_tensor = quantized_state_dict[name]\n",
        "            current_delta = layer_deltas[name]\n",
        "            wq = torch.round(w_tensor / current_delta)\n",
        "            wq_clamp = torch.clamp(wq, max=Max_A, min=Max_B)\n",
        "            quantized_state_dict[name] = wq_clamp\n",
        "\n",
        "            # Calculate bias sum for FBR\n",
        "            bias_tensor = (wq - wq_clamp).abs()\n",
        "            current_bias_sum = torch.sum(bias_tensor, dim=(1,2,3))\n",
        "\n",
        "        elif name.startswith('layer'):\n",
        "            # Handle BN parameters for layers (layer1, layer2, etc.)\n",
        "            # Assumes these come AFTER their corresponding adder layer\n",
        "            if name.endswith('running_mean'):\n",
        "                m_tensor = quantized_state_dict[name]\n",
        "                mq = torch.round(m_tensor / current_delta)\n",
        "                quantized_state_dict[name] = mq + current_bias_sum\n",
        "\n",
        "            elif name.endswith('bias') and 'bn' in name:\n",
        "                x_tensor = quantized_state_dict[name]\n",
        "                xq_tensor = x_tensor / current_delta\n",
        "                quantized_state_dict[name] = xq_tensor\n",
        "\n",
        "    # Quantize FC\n",
        "    quantized_state_dict['fc.weight'] = quantized_state_dict['fc.weight'] * delta_last\n",
        "\n",
        "    # Evaluate\n",
        "    quant_model = AdderNet2_0(num_classes=10).to(device)\n",
        "    quant_model.load_manual_weights(quantized_state_dict)\n",
        "    quant_model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in testloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            outputs = quant_model(inputs)\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += targets.size(0)\n",
        "            correct += predicted.eq(targets).sum().item()\n",
        "\n",
        "    accuracy = 100. * correct / total\n",
        "    return accuracy\n",
        "\n",
        "def run_optimization():\n",
        "    print(\"-+\" * 25)\n",
        "    print(\"Starting DE Optimization for AdderNet2.0 Quantization\")\n",
        "    print(\"-+\" * 25)\n",
        "\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load Data\n",
        "    testloader = load_data(batch_size=200)\n",
        "\n",
        "    # Load Pretrained Model Weights\n",
        "    print('Loading pretrained weights...')\n",
        "    #model_dir = '/kaggle/input/state-dictionary'\n",
        "    # files = os.listdir(model_dir)\n",
        "    #model_path = os.path.join(model_dir, 'AdderNet_model.pth')\n",
        "    model_path = 'AdderNet_model.pth'\n",
        "\n",
        "    # Load state dictionary\n",
        "    checkpoint = torch.load(model_path, map_location=device)\n",
        "    if 'net' in checkpoint:\n",
        "        state_dict_raw = checkpoint['net']\n",
        "    else:\n",
        "        state_dict_raw = checkpoint\n",
        "\n",
        "    def remap_key(key):\n",
        "        \"\"\"Map original checkpoint keys to the correct AdderNet naming.\"\"\"\n",
        "        new_key = key.replace('module.', '')\n",
        "\n",
        "        # conv and batchnorm generic\n",
        "        if new_key.startswith('conv1.') or new_key.startswith('bn1.') or new_key.startswith('fc.') or new_key.startswith('bn2.'):\n",
        "            return new_key\n",
        "\n",
        "        # Process residual layers\n",
        "        for layer_num in [1, 2, 3]:\n",
        "            prefix = f'layer{layer_num}.'\n",
        "            if new_key.startswith(prefix):\n",
        "                rest = new_key[len(prefix):]  # everything after 'layerX.'\n",
        "\n",
        "                # If next is block index\n",
        "                if len(rest) > 0 and rest[0].isdigit():\n",
        "                    dot_idx = rest.find('.')\n",
        "                    if dot_idx != -1:\n",
        "                         block_num = rest[:dot_idx]\n",
        "                         rest_after_block = rest[dot_idx+1:]\n",
        "\n",
        "                         # Handle downsample case\n",
        "                         if rest_after_block.startswith('downsample.'):\n",
        "                             ds_rest = rest_after_block[len('downsample.'):]\n",
        "                             if ds_rest.startswith('0.'):\n",
        "                                 # conv -> adder\n",
        "                                 return f'layer{layer_num}.downsample_adder.{ds_rest[2:]}'\n",
        "                             elif ds_rest.startswith('1.'):\n",
        "                                 return f'layer{layer_num}.downsample_bn.{ds_rest[2:]}'\n",
        "\n",
        "                         # Otherwise: normal residual block conv/bn -> adder/bn\n",
        "                         # Check if it is conv1/2 or bn1/2\n",
        "                         # In original: conv1 -> adder1, conv2 -> adder2\n",
        "                         if 'conv1.' in rest_after_block:\n",
        "                             rest_after_block = rest_after_block.replace('conv1.', 'adder1.')\n",
        "                         elif 'conv2.' in rest_after_block:\n",
        "                             rest_after_block = rest_after_block.replace('conv2.', 'adder2.')\n",
        "\n",
        "                         return f'layer{layer_num}.blocks.{block_num}.{rest_after_block}'\n",
        "\n",
        "        return new_key\n",
        "\n",
        "    # Apply remapping\n",
        "    fixed_state_dict = {}\n",
        "    for k, v in state_dict_raw.items():\n",
        "        fixed_key = remap_key(k)\n",
        "        fixed_state_dict[fixed_key] = v\n",
        "\n",
        "    # Prepare keys\n",
        "    print(\"Debug: First 10 keys in fixed_state_dict:\")\n",
        "    for key in list(fixed_state_dict.keys())[:10]:\n",
        "        print(key)\n",
        "\n",
        "    adder_keys = get_adder_layer_keys(fixed_state_dict)\n",
        "    print(f\"Found {len(adder_keys)} adder layers to optimize.\")\n",
        "    if len(adder_keys) == 0:\n",
        "        print(\"Error: No adder layers found. Check model keys or filtering logic.\")\n",
        "        return\n",
        "\n",
        "    # DE Parameters\n",
        "    n_dim = 20\n",
        "    size_pop = 50\n",
        "    max_iter = 50\n",
        "    prob_mut = 0.85 # Also called CR\n",
        "    F = 0.4\n",
        "\n",
        "    #lb = [2.0] * 20\n",
        "    #ub = [4.0] * 20\n",
        "\n",
        "    # Activation value range: using best results from previous generations\n",
        "    lb = [2.4, 2.73, 2.63, 2.4, 2.61, 2.45, 2.53, 2.48, 3.09, 2.5, 2.13, 2.36, 2.0, 2.52, 2.18, 2.0, 2.29, 2.21, 2.0, 2.38]\n",
        "    ub = [2.4, 2.74, 2.67, 2.4, 2.61, 2.45, 2.68, 2.48, 3.10, 2.5, 2.13, 2.36, 2.0, 2.52, 2.18, 2.0, 2.29, 2.21, 2.0, 2.38]\n",
        "\n",
        "    bit_array = [4] # Bits to be tested (unsigned integer 4)\n",
        "\n",
        "    for bits in bit_array:\n",
        "        print(f\"\\nOptimizing for {bits}-bit quantization...\")\n",
        "\n",
        "        # Define objective wrapper\n",
        "        def objective(x):\n",
        "            acc = quantization_objective(x, fixed_state_dict, adder_keys, bits, device, testloader)\n",
        "            return acc # maximizing accuracy\n",
        "\n",
        "        de = DE(objective, F, lb, ub, size_pop, n_dim, max_iter, prob_mut)\n",
        "\n",
        "        best_x, best_acc = de.run()\n",
        "\n",
        "        print(f\"Best Max Vals for {bits}-bit: {best_x}\")\n",
        "        print(f\"Best Accuracy: {best_acc:.2f}%\")\n",
        "        print(\"-+\" * 25)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_optimization()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2026-02-17T23:25:38.383681Z",
          "iopub.execute_input": "2026-02-17T23:25:38.384071Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "83S-0xE53wHO",
        "outputId": "1ae4c08e-9ea5-43a5-8b17-7e8905f683f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
            "Starting DE Optimization for AdderNet2.0 Quantization\n",
            "-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n",
            "Using device: cuda\n",
            "Preparing data..\n",
            "Loading pretrained weights...\n",
            "Debug: First 10 keys in fixed_state_dict:\n",
            "conv1.weight\n",
            "bn1.weight\n",
            "bn1.bias\n",
            "bn1.running_mean\n",
            "bn1.running_var\n",
            "bn1.num_batches_tracked\n",
            "layer1.blocks.0.adder1.adder\n",
            "layer1.blocks.0.bn1.weight\n",
            "layer1.blocks.0.bn1.bias\n",
            "layer1.blocks.0.bn1.running_mean\n",
            "Found 20 adder layers to optimize.\n",
            "\n",
            "Optimizing for 4-bit quantization...\n",
            "start run\n",
            "trial: [2.4  2.73 2.66 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 0 accuracy: 89.19%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.59 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 1 accuracy: 89.18%\n",
            "trial: [2.4  2.74 2.65 2.4  2.61 2.45 2.53 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 2 accuracy: 88.99%\n",
            "trial: [2.4  2.73 2.67 2.4  2.61 2.45 2.65 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 3 accuracy: 88.77%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.69 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 4 accuracy: 89.12%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.59 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 5 accuracy: 89.20%\n",
            "trial: [2.4  2.73 2.67 2.4  2.61 2.45 2.68 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 6 accuracy: 88.97%\n",
            "trial: [2.4  2.73 2.66 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 7 accuracy: 89.19%\n",
            "trial: [2.4  2.73 2.63 2.4  2.61 2.45 2.58 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 8 accuracy: 89.13%\n",
            "trial: [2.4  2.74 2.67 2.4  2.61 2.45 2.6  2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 9 accuracy: 88.94%\n",
            "trial: [2.4  2.74 2.63 2.4  2.61 2.45 2.65 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 10 accuracy: 89.00%\n",
            "trial: [2.4  2.73 2.65 2.4  2.61 2.45 2.64 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 11 accuracy: 88.91%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.66 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 12 accuracy: 89.02%\n",
            "trial: [2.4  2.74 2.65 2.4  2.61 2.45 2.66 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 13 accuracy: 89.11%\n",
            "trial: [2.4  2.74 2.65 2.4  2.61 2.45 2.56 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 14 accuracy: 89.23%\n",
            "trial: [2.4  2.74 2.63 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 15 accuracy: 89.16%\n",
            "trial: [2.4  2.74 2.67 2.4  2.61 2.45 2.55 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 16 accuracy: 89.12%\n",
            "trial: [2.4  2.73 2.66 2.4  2.61 2.45 2.68 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 17 accuracy: 89.18%\n",
            "trial: [2.4  2.74 2.65 2.4  2.61 2.45 2.64 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 18 accuracy: 88.95%\n",
            "trial: [2.4  2.74 2.63 2.4  2.61 2.45 2.64 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 19 accuracy: 88.93%\n",
            "trial: [2.4  2.74 2.67 2.4  2.61 2.45 2.56 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 20 accuracy: 89.00%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.63 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 21 accuracy: 88.94%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.62 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 22 accuracy: 89.20%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.65 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 23 accuracy: 89.00%\n",
            "trial: [2.4  2.74 2.63 2.4  2.61 2.45 2.62 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 24 accuracy: 89.19%\n",
            "trial: [2.4  2.74 2.67 2.4  2.61 2.45 2.64 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 25 accuracy: 88.73%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.56 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 26 accuracy: 89.09%\n",
            "trial: [2.4  2.74 2.67 2.4  2.61 2.45 2.57 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 27 accuracy: 88.98%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.66 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 28 accuracy: 89.13%\n",
            "trial: [2.4  2.74 2.65 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 29 accuracy: 89.22%\n",
            "trial: [2.4  2.74 2.65 2.4  2.61 2.45 2.53 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 30 accuracy: 88.99%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.55 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 31 accuracy: 89.32%\n",
            "trial: [2.4  2.74 2.65 2.4  2.61 2.45 2.62 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 32 accuracy: 89.13%\n",
            "trial: [2.4  2.73 2.67 2.4  2.61 2.45 2.64 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 33 accuracy: 88.71%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.53 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 34 accuracy: 89.00%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.66 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 35 accuracy: 89.02%\n",
            "trial: [2.4  2.74 2.65 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 36 accuracy: 89.19%\n",
            "trial: [2.4  2.74 2.65 2.4  2.61 2.45 2.53 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 37 accuracy: 88.99%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 38 accuracy: 89.21%\n",
            "trial: [2.4  2.74 2.63 2.4  2.61 2.45 2.59 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 39 accuracy: 89.21%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.65 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 40 accuracy: 88.99%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 41 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.63 2.4  2.61 2.45 2.64 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 42 accuracy: 89.08%\n",
            "trial: [2.4  2.74 2.63 2.4  2.61 2.45 2.56 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 43 accuracy: 89.24%\n",
            "trial: [2.4  2.74 2.63 2.4  2.61 2.45 2.63 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 44 accuracy: 88.91%\n",
            "trial: [2.4  2.73 2.67 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 45 accuracy: 88.99%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.57 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 46 accuracy: 89.19%\n",
            "trial: [2.4  2.74 2.65 2.4  2.61 2.45 2.56 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 47 accuracy: 89.23%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.53 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 48 accuracy: 88.98%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.61 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 49 accuracy: 89.01%\n",
            "Generation 1: Best Accuracy = 89.50%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.59 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 0 accuracy: 89.18%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 1 accuracy: 89.21%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.65 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 2 accuracy: 89.00%\n",
            "trial: [2.4  2.73 2.65 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 3 accuracy: 89.17%\n",
            "trial: [2.4  2.73 2.67 2.4  2.61 2.45 2.68 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 4 accuracy: 88.97%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 5 accuracy: 89.32%\n",
            "trial: [2.4  2.74 2.65 2.4  2.61 2.45 2.65 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 6 accuracy: 88.99%\n",
            "trial: [2.4  2.74 2.63 2.4  2.61 2.45 2.67 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 7 accuracy: 89.28%\n",
            "trial: [2.4  2.74 2.63 2.4  2.61 2.45 2.55 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 8 accuracy: 89.20%\n",
            "trial: [2.4  2.74 2.63 2.4  2.61 2.45 2.6  2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 9 accuracy: 89.17%\n",
            "trial: [2.4  2.74 2.65 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 10 accuracy: 89.22%\n",
            "trial: [2.4  2.74 2.65 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 11 accuracy: 89.19%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.59 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 12 accuracy: 89.22%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 13 accuracy: 89.31%\n",
            "trial: [2.4  2.73 2.63 2.4  2.61 2.45 2.62 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 14 accuracy: 89.07%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.55 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 15 accuracy: 89.19%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 16 accuracy: 89.21%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.53 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 17 accuracy: 89.06%\n",
            "trial: [2.4  2.74 2.65 2.4  2.61 2.45 2.53 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 18 accuracy: 88.99%\n",
            "trial: [2.4  2.74 2.63 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 19 accuracy: 89.23%\n",
            "trial: [2.4  2.74 2.67 2.4  2.61 2.45 2.62 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 20 accuracy: 88.87%\n",
            "trial: [2.4  2.74 2.65 2.4  2.61 2.45 2.65 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 21 accuracy: 88.99%\n",
            "trial: [2.4  2.73 2.63 2.4  2.61 2.45 2.67 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 22 accuracy: 89.24%\n",
            "trial: [2.4  2.74 2.65 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 23 accuracy: 89.19%\n",
            "trial: [2.4  2.73 2.66 2.4  2.61 2.45 2.63 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 24 accuracy: 89.16%\n",
            "trial: [2.4  2.73 2.63 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 25 accuracy: 89.17%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.63 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 26 accuracy: 89.02%\n",
            "trial: [2.4  2.74 2.65 2.4  2.61 2.45 2.62 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 27 accuracy: 89.15%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.6  2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 28 accuracy: 89.24%\n",
            "trial: [2.4  2.74 2.65 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 29 accuracy: 89.22%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.62 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 30 accuracy: 89.31%\n",
            "trial: [2.4  2.74 2.65 2.4  2.61 2.45 2.53 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 31 accuracy: 88.99%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.56 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 32 accuracy: 89.28%\n",
            "trial: [2.4  2.73 2.63 2.4  2.61 2.45 2.67 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 33 accuracy: 89.24%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 34 accuracy: 89.18%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.61 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 35 accuracy: 89.05%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.56 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 36 accuracy: 89.28%\n",
            "trial: [2.4  2.73 2.65 2.4  2.61 2.45 2.66 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 37 accuracy: 89.13%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 38 accuracy: 89.31%\n",
            "trial: [2.4  2.74 2.63 2.4  2.61 2.45 2.67 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 39 accuracy: 89.28%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.54 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 40 accuracy: 88.93%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.67 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 41 accuracy: 89.13%\n",
            "trial: [2.4  2.73 2.65 2.4  2.61 2.45 2.64 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 42 accuracy: 88.91%\n",
            "trial: [2.4  2.73 2.66 2.4  2.61 2.45 2.69 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 43 accuracy: 89.06%\n",
            "trial: [2.4  2.74 2.68 2.4  2.61 2.45 2.59 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 44 accuracy: 88.87%\n",
            "trial: [2.4  2.73 2.65 2.4  2.61 2.45 2.69 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 45 accuracy: 89.21%\n",
            "trial: [2.4  2.73 2.67 2.4  2.61 2.45 2.67 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 46 accuracy: 88.97%\n",
            "trial: [2.4  2.74 2.65 2.4  2.61 2.45 2.68 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 47 accuracy: 89.20%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 48 accuracy: 89.27%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.61 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 49 accuracy: 89.05%\n",
            "Generation 2: Best Accuracy = 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.68 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 0 accuracy: 89.28%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.58 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 1 accuracy: 89.12%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.68 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 2 accuracy: 89.28%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.58 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 3 accuracy: 89.02%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.69 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 4 accuracy: 89.12%\n",
            "trial: [2.4  2.74 2.67 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 5 accuracy: 88.93%\n",
            "trial: [2.4  2.73 2.63 2.4  2.61 2.45 2.63 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 6 accuracy: 89.02%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.65 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 7 accuracy: 89.14%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.53 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 8 accuracy: 89.06%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.68 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 9 accuracy: 89.16%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.68 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 10 accuracy: 89.16%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.56 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 11 accuracy: 89.14%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.56 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 12 accuracy: 89.14%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 13 accuracy: 89.31%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.65 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 14 accuracy: 89.11%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.55 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 15 accuracy: 89.30%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.53 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 16 accuracy: 89.06%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.68 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 17 accuracy: 89.28%\n",
            "trial: [2.4  2.74 2.67 2.4  2.61 2.45 2.64 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 18 accuracy: 88.73%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.59 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 19 accuracy: 89.24%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.63 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 20 accuracy: 89.02%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.66 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 21 accuracy: 89.12%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.6  2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 22 accuracy: 89.24%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 23 accuracy: 89.18%\n",
            "trial: [2.4  2.73 2.63 2.4  2.61 2.45 2.59 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 24 accuracy: 89.07%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 25 accuracy: 89.27%\n",
            "trial: [2.4  2.74 2.63 2.4  2.61 2.45 2.59 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 26 accuracy: 89.27%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.65 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 27 accuracy: 88.99%\n",
            "trial: [2.4  2.74 2.63 2.4  2.61 2.45 2.59 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 28 accuracy: 89.21%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.64 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 29 accuracy: 88.98%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.62 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 30 accuracy: 89.31%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.64 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 31 accuracy: 88.98%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.61 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 32 accuracy: 89.05%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 33 accuracy: 89.50%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.66 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 34 accuracy: 89.14%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.54 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 35 accuracy: 88.93%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.56 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 36 accuracy: 89.14%\n",
            "trial: [2.4  2.74 2.65 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 37 accuracy: 89.22%\n",
            "trial: [2.4  2.73 2.66 2.4  2.61 2.45 2.67 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 38 accuracy: 89.29%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 39 accuracy: 89.31%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 40 accuracy: 89.32%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 41 accuracy: 89.32%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.6  2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 42 accuracy: 89.18%\n",
            "trial: [2.4  2.74 2.65 2.4  2.61 2.45 2.65 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 43 accuracy: 88.99%\n",
            "trial: [2.4  2.73 2.63 2.4  2.61 2.45 2.67 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 44 accuracy: 89.24%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.58 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 45 accuracy: 89.12%\n",
            "trial: [2.4  2.74 2.65 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 46 accuracy: 89.22%\n",
            "trial: [2.4  2.74 2.65 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 47 accuracy: 89.22%\n",
            "trial: [2.4  2.74 2.65 2.4  2.61 2.45 2.65 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 48 accuracy: 88.99%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.64 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 49 accuracy: 88.98%\n",
            "Generation 3: Best Accuracy = 89.50%\n",
            "trial: [2.4  2.73 2.65 2.4  2.61 2.45 2.65 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 0 accuracy: 88.89%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.6  2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 1 accuracy: 89.24%\n",
            "trial: [2.4  2.74 2.63 2.4  2.61 2.45 2.64 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 2 accuracy: 89.01%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 3 accuracy: 89.27%\n",
            "trial: [2.4  2.74 2.65 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 4 accuracy: 89.22%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 5 accuracy: 89.31%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 6 accuracy: 89.50%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.68 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 7 accuracy: 89.16%\n",
            "trial: [2.4  2.74 2.63 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 8 accuracy: 89.16%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 9 accuracy: 89.18%\n",
            "trial: [2.4  2.74 2.67 2.4  2.61 2.45 2.6  2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 10 accuracy: 88.89%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 11 accuracy: 89.32%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.6  2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 12 accuracy: 89.24%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 13 accuracy: 89.21%\n",
            "trial: [2.4  2.74 2.63 2.4  2.61 2.45 2.58 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 14 accuracy: 89.01%\n",
            "trial: [2.4  2.74 2.67 2.4  2.61 2.45 2.6  2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 15 accuracy: 88.89%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.65 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 16 accuracy: 89.00%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 17 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.65 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 18 accuracy: 89.17%\n",
            "trial: [2.4  2.73 2.65 2.4  2.61 2.45 2.66 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 19 accuracy: 89.10%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.65 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 20 accuracy: 89.00%\n",
            "trial: [2.4  2.74 2.63 2.4  2.61 2.45 2.62 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 21 accuracy: 89.19%\n",
            "trial: [2.4  2.73 2.63 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 22 accuracy: 89.17%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.62 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 23 accuracy: 89.20%\n",
            "trial: [2.4  2.74 2.67 2.4  2.61 2.45 2.64 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 24 accuracy: 88.73%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.6  2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 25 accuracy: 89.09%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.65 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 26 accuracy: 89.11%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 27 accuracy: 89.31%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.59 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 28 accuracy: 89.18%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.59 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 29 accuracy: 89.24%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.59 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 30 accuracy: 89.18%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 31 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.65 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 32 accuracy: 89.11%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 33 accuracy: 89.18%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 34 accuracy: 89.50%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.55 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 35 accuracy: 89.19%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 36 accuracy: 89.31%\n",
            "trial: [2.4  2.74 2.65 2.4  2.61 2.45 2.66 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 37 accuracy: 89.00%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.66 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 38 accuracy: 89.14%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 39 accuracy: 89.18%\n",
            "trial: [2.4  2.74 2.65 2.4  2.61 2.45 2.63 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 40 accuracy: 89.03%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 41 accuracy: 89.18%\n",
            "trial: [2.4  2.74 2.63 2.4  2.61 2.45 2.6  2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 42 accuracy: 89.14%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 43 accuracy: 89.32%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 44 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.65 2.4  2.61 2.45 2.64 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 45 accuracy: 88.91%\n",
            "trial: [2.4  2.74 2.65 2.4  2.61 2.45 2.58 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 46 accuracy: 89.01%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.58 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 47 accuracy: 89.12%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.57 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 48 accuracy: 89.23%\n",
            "trial: [2.4  2.74 2.63 2.4  2.61 2.45 2.54 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 49 accuracy: 89.08%\n",
            "Generation 4: Best Accuracy = 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 0 accuracy: 89.18%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.6  2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 1 accuracy: 89.09%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.58 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 2 accuracy: 89.12%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 3 accuracy: 89.18%\n",
            "trial: [2.4  2.74 2.65 2.4  2.61 2.45 2.56 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 4 accuracy: 89.23%\n",
            "trial: [2.4  2.73 2.65 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 5 accuracy: 89.21%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.6  2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 6 accuracy: 89.24%\n",
            "trial: [2.4  2.74 2.65 2.4  2.61 2.45 2.62 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 7 accuracy: 89.15%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.61 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 8 accuracy: 89.05%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 9 accuracy: 89.31%\n",
            "trial: [2.4  2.74 2.65 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 10 accuracy: 89.22%\n",
            "trial: [2.4  2.74 2.63 2.4  2.61 2.45 2.64 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 11 accuracy: 88.93%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 12 accuracy: 89.27%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.66 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 13 accuracy: 89.12%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.54 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 14 accuracy: 88.90%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.62 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 15 accuracy: 89.23%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.62 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 16 accuracy: 89.31%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 17 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.68 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 18 accuracy: 89.28%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.59 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 19 accuracy: 88.96%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 20 accuracy: 89.31%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.57 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 21 accuracy: 89.14%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.6  2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 22 accuracy: 89.09%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.65 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 23 accuracy: 89.14%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.56 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 24 accuracy: 89.14%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 25 accuracy: 89.50%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 26 accuracy: 89.27%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 27 accuracy: 89.31%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 28 accuracy: 89.50%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 29 accuracy: 89.27%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 30 accuracy: 89.18%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.67 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 31 accuracy: 89.13%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.62 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 32 accuracy: 89.31%\n",
            "trial: [2.4  2.73 2.63 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 33 accuracy: 88.98%\n",
            "trial: [2.4  2.73 2.65 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 34 accuracy: 89.17%\n",
            "trial: [2.4  2.73 2.63 2.4  2.61 2.45 2.65 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 35 accuracy: 89.07%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.64 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 36 accuracy: 88.92%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 37 accuracy: 89.21%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.62 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 38 accuracy: 89.31%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 39 accuracy: 89.50%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.56 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 40 accuracy: 89.14%\n",
            "trial: [2.4  2.74 2.65 2.4  2.61 2.45 2.62 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 41 accuracy: 89.15%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 42 accuracy: 89.50%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 43 accuracy: 89.32%\n",
            "trial: [2.4  2.74 2.63 2.4  2.61 2.45 2.62 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 44 accuracy: 89.19%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.69 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 45 accuracy: 89.25%\n",
            "trial: [2.4  2.74 2.63 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 46 accuracy: 89.16%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.63 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 47 accuracy: 88.94%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 48 accuracy: 89.27%\n",
            "trial: [2.4  2.73 2.65 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 49 accuracy: 89.17%\n",
            "Generation 5: Best Accuracy = 89.50%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.58 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 0 accuracy: 89.15%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 1 accuracy: 89.18%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.62 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 2 accuracy: 89.20%\n",
            "trial: [2.4  2.74 2.65 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 3 accuracy: 89.19%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 4 accuracy: 89.18%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.64 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 5 accuracy: 88.98%\n",
            "trial: [2.4  2.73 2.65 2.4  2.61 2.45 2.67 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 6 accuracy: 89.12%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 7 accuracy: 89.32%\n",
            "trial: [2.4  2.74 2.67 2.4  2.61 2.45 2.64 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 8 accuracy: 88.73%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 9 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 10 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.66 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 11 accuracy: 89.12%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.62 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 12 accuracy: 89.20%\n",
            "trial: [2.4  2.74 2.65 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 13 accuracy: 89.19%\n",
            "trial: [2.4  2.73 2.65 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 14 accuracy: 89.21%\n",
            "trial: [2.4  2.73 2.63 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 15 accuracy: 89.17%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 16 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 17 accuracy: 89.18%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 18 accuracy: 89.21%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 19 accuracy: 89.32%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 20 accuracy: 89.50%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.68 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 21 accuracy: 89.31%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 22 accuracy: 89.50%\n",
            "trial: [2.4  2.74 2.65 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 23 accuracy: 89.19%\n",
            "trial: [2.4  2.74 2.63 2.4  2.61 2.45 2.67 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 24 accuracy: 89.28%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 25 accuracy: 89.31%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.59 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 26 accuracy: 88.96%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 27 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.65 2.4  2.61 2.45 2.66 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 28 accuracy: 89.13%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 29 accuracy: 89.27%\n",
            "trial: [2.4  2.74 2.65 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 30 accuracy: 89.22%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.65 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 31 accuracy: 89.11%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 32 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 33 accuracy: 89.50%\n",
            "trial: [2.4  2.74 2.65 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 34 accuracy: 89.22%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 35 accuracy: 89.27%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 36 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.65 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 37 accuracy: 89.21%\n",
            "trial: [2.4  2.73 2.66 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 38 accuracy: 89.19%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 39 accuracy: 89.18%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.64 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 40 accuracy: 89.10%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 41 accuracy: 89.32%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 42 accuracy: 89.32%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 43 accuracy: 89.32%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 44 accuracy: 89.21%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 45 accuracy: 89.50%\n",
            "trial: [2.4  2.74 2.65 2.4  2.61 2.45 2.56 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 46 accuracy: 89.23%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 47 accuracy: 89.32%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 48 accuracy: 89.18%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 49 accuracy: 89.32%\n",
            "Generation 6: Best Accuracy = 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 0 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 1 accuracy: 89.50%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.62 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 2 accuracy: 89.31%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 3 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 4 accuracy: 89.50%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 5 accuracy: 89.32%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 6 accuracy: 89.18%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 7 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.65 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 8 accuracy: 89.11%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.65 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 9 accuracy: 89.11%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 10 accuracy: 89.50%\n",
            "trial: [2.4  2.74 2.65 2.4  2.61 2.45 2.62 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 11 accuracy: 89.15%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 12 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 13 accuracy: 89.50%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 14 accuracy: 89.32%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 15 accuracy: 89.50%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.62 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 16 accuracy: 89.31%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 17 accuracy: 89.27%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 18 accuracy: 89.50%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 19 accuracy: 89.32%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 20 accuracy: 89.50%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 21 accuracy: 89.31%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 22 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.62 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 23 accuracy: 89.20%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 24 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 25 accuracy: 89.50%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.68 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 26 accuracy: 89.31%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.62 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 27 accuracy: 89.31%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 28 accuracy: 89.27%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.65 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 29 accuracy: 89.11%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.62 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 30 accuracy: 89.23%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 31 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 32 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.68 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 33 accuracy: 89.28%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 34 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 35 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 36 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 37 accuracy: 89.50%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.65 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 38 accuracy: 89.00%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 39 accuracy: 89.50%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 40 accuracy: 89.32%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 41 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.65 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 42 accuracy: 89.21%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 43 accuracy: 89.50%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 44 accuracy: 89.32%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 45 accuracy: 89.32%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 46 accuracy: 89.50%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.65 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 47 accuracy: 88.99%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 48 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 49 accuracy: 89.50%\n",
            "Generation 7: Best Accuracy = 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 0 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 1 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 2 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 3 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 4 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 5 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 6 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 7 accuracy: 89.50%\n",
            "trial: [2.4  2.74 2.65 2.4  2.61 2.45 2.68 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 8 accuracy: 89.20%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 9 accuracy: 89.50%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.68 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 10 accuracy: 89.27%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 11 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.65 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 12 accuracy: 89.21%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 13 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 14 accuracy: 89.50%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 15 accuracy: 89.32%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 16 accuracy: 89.50%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.68 2.48 3.09 2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 17 accuracy: 89.31%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 18 accuracy: 89.50%\n",
            "trial: [2.4  2.74 2.66 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 19 accuracy: 89.31%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 20 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 21 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 22 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 23 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 24 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 25 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 26 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 27 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 28 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 29 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 30 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 31 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 32 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 33 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 34 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 35 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 36 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 37 accuracy: 89.50%\n",
            "trial: [2.4  2.74 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 38 accuracy: 89.32%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 39 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 40 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 41 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 42 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 43 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 44 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 45 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 46 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 47 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 48 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 49 accuracy: 89.50%\n",
            "Generation 8: Best Accuracy = 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 0 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 1 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 2 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 3 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 4 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 5 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 6 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 7 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 8 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 9 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 10 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 11 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 12 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 13 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 14 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 15 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 16 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 17 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 18 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 19 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 20 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 21 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 22 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 23 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n",
            "Trial 24 accuracy: 89.50%\n",
            "trial: [2.4  2.73 2.64 2.4  2.61 2.45 2.67 2.48 3.1  2.5  2.13 2.36 2.   2.52\n",
            " 2.18 2.   2.29 2.21 2.   2.38]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3290570666.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m     \u001b[0mrun_optimization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3290570666.py\u001b[0m in \u001b[0;36mrun_optimization\u001b[0;34m()\u001b[0m\n\u001b[1;32m    217\u001b[0m         \u001b[0;31m# de.X = np.clip(de.X, lb, ub)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m         \u001b[0mbest_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mde\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Best Max Vals for {bits}-bit: {best_x}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1831943050.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, max_iter)\u001b[0m\n\u001b[1;32m    187\u001b[0m                      \u001b[0mobj_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m                      \u001b[0mobj_trial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Trial {j} accuracy: {obj_trial:.2f}%\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3290570666.py\u001b[0m in \u001b[0;36mobjective\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;31m# Define objective wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mobjective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m             \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquantization_objective\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfixed_state_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madder_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0macc\u001b[0m \u001b[0;31m# maximizing accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3290570666.py\u001b[0m in \u001b[0;36mquantization_objective\u001b[0;34m(x, fixed_state_dict, adder_keys, bits, device, testloader)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquant_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    731\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 732\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    733\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    786\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchvision/datasets/cifar.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;31m# doing this so that it is consistent with all other datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# to return a PIL Image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   3334\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3336\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrombuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"raw\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrawmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3338\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfrombuffer\u001b[0;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m   3223\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3224\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3225\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfrombytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mfrombytes\u001b[0;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m   3150\u001b[0m     \u001b[0m_check_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3152\u001b[0;31m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3153\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3154\u001b[0m         \u001b[0mdecoder_args\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mnew\u001b[0;34m(mode, size, color)\u001b[0m\n\u001b[1;32m   3115\u001b[0m             \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpalette\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImagePalette\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImagePalette\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3116\u001b[0m             \u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpalette\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcolor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolor_ints\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3117\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "execution_count": 24
    }
  ]
}